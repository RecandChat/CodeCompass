{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install the latest version of the openai library\n",
    "!pip install openai -q --upgrade\n",
    "\n",
    "# imports\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from openai.types.beta.assistant import Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openai_key(file_path='../../secrets/openAI_key') -> str:\n",
    "    \"\"\"\n",
    "    Loads the OpenAI API key from a specified file.\n",
    "\n",
    "    :param file_path: The path to the file containing the OpenAI API key.\n",
    "    :return: The OpenAI API key, or an empty string if the file cannot be found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            openai_key = file.read().strip()  # Use strip() to remove newline characters\n",
    "            print(\"OpenAI key loaded successfully.\")\n",
    "            return openai_key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"OpenAI key file not found at {file_path}.\")\n",
    "        return \"\"\n",
    "\n",
    "def load_github_token(file_path='../../secrets/github_token') -> str:\n",
    "    \"\"\"\n",
    "    Loads the GitHub token from a specified file.\n",
    "\n",
    "    :param file_path: The path to the file containing the GitHub token.\n",
    "    :return: The GitHub token, or an empty string if the file cannot be found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            github_token = file.read().strip()  # Use strip() to remove newline characters\n",
    "            print(\"GitHub token loaded successfully.\")\n",
    "            return github_token\n",
    "    except FileNotFoundError:\n",
    "        print(f\"GitHub token file not found at {file_path}.\")\n",
    "        return \"\"\n",
    "    \n",
    "def load_askthecode_api_base_url(file_path= '../../secrets/askthecode_API') -> str:\n",
    "    \"\"\"\n",
    "    Loads the AskTheCode API base URL from a local secrets file.\n",
    "\n",
    "    :return: The base URL as a string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        base_url = file.read().strip()\n",
    "    return base_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API request helper functions\n",
    "from typing import Dict, Any, Union, List, Optional\n",
    "\n",
    "def remove_useful_urls(response_json: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Removes the 'usefulUrls' key from the API response JSON object if present.\n",
    "\n",
    "    :param response_json: A dictionary representing the JSON response from an API.\n",
    "    :return: The modified dictionary with the 'usefulUrls' key removed if it was present.\n",
    "    \"\"\"\n",
    "    if 'usefulUrls' in response_json:\n",
    "        del response_json['usefulUrls']\n",
    "    return response_json\n",
    "\n",
    "\n",
    "def make_api_request(endpoint_url: str, params: Dict[str, Any], github_token: str) -> Union[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Makes a POST request to the specified API endpoint with given parameters and returns the processed JSON response.\n",
    "    Removes the 'usefulUrls' field from the response if it exists. In case of an error, returns an error message.\n",
    "\n",
    "    :param endpoint_url: The URL of the API endpoint to which the request is made.\n",
    "    :param params: A dictionary of parameters to be sent in the request.\n",
    "    :param github_token: The GitHub token used for Authorization header.\n",
    "    :return: A dictionary representing the JSON response from the API or a string containing an error message.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {github_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint_url, json=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        response_json = remove_useful_urls(response_json)\n",
    "        return response_json\n",
    "    else:\n",
    "        return f\"Failed to communicate with the API: {response.status_code}, Reason: {response.reason}\"\n",
    "\n",
    "\"\"\"\n",
    "ASK THE CODE API functions\n",
    "\"\"\"\n",
    "\n",
    "def get_repo_structure(url: str, branch: Optional[str] = None, relativePaths: Optional[List[str]] = None) -> Union[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Retrieves the structure of a GitHub repository at the specified URL, optionally filtering by branch and relative paths.\n",
    "\n",
    "    :param url: The URL of the GitHub repository.\n",
    "    :param branch: Optional; the specific branch to get the structure from.\n",
    "    :param relativePaths: Optional; specific relative paths within the repository to include in the structure.\n",
    "    :return: A dictionary representing the structure of the repository or an error message.\n",
    "    \"\"\"\n",
    "    base_url = load_askthecode_api_base_url()\n",
    "    get_repo_structure_url = f\"{base_url}/api/repository/structure\"\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'branch': branch,\n",
    "        'relativePaths': relativePaths\n",
    "    }\n",
    "    return make_api_request(get_repo_structure_url, params, github_token)\n",
    "\n",
    "# get repo content\n",
    "def get_repo_content(url: str, filePaths: List[str], branch: Optional[str] = None, relativePath: Optional[str] = None) -> Union[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Retrieves the content of specific files within a GitHub repository, optionally filtered by branch and a relative path.\n",
    "\n",
    "    :param url: The URL of the GitHub repository.\n",
    "    :param filePaths: The list of file paths within the repository whose content is to be retrieved.\n",
    "    :param branch: Optional; the specific branch to get the content from.\n",
    "    :param relativePath: Optional; the specific relative path within the repository to filter the file paths.\n",
    "    :return: A dictionary representing the content of the specified files or an error message.\n",
    "    \"\"\"\n",
    "    base_url = load_askthecode_api_base_url()\n",
    "    get_repo_content_url = f\"{base_url}/api/repository/content\"\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'filePaths': filePaths,\n",
    "        'branch': branch,\n",
    "        'relativePath': relativePath\n",
    "    }\n",
    "    return make_api_request(get_repo_content_url, params, github_token)\n",
    "    \n",
    "\n",
    "# get repo branches\n",
    "def get_repo_branches(url: str) -> Union[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Retrieves the list of branches for the specified GitHub repository.\n",
    "\n",
    "    :param url: The URL of the GitHub repository.\n",
    "    :return: A dictionary containing the list of branches or an error message.\n",
    "    \"\"\"\n",
    "    base_url = load_askthecode_api_base_url()\n",
    "    get_repo_branches_url = f\"{base_url}/api/repository/branch/list\"\n",
    "    params = {'url': url}\n",
    "    return make_api_request(get_repo_branches_url, params, github_token)\n",
    "   \n",
    "    \n",
    "# Get commit history\n",
    "def get_commit_history(url: str, branch: Optional[str] = None, filePath: Optional[str] = None) -> Union[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Retrieves the commit history for a specified file or branch within a GitHub repository.\n",
    "\n",
    "    :param url: The URL of the GitHub repository.\n",
    "    :param branch: Optional; the specific branch to retrieve the commit history from.\n",
    "    :param filePath: Optional; the specific file path to retrieve the commit history for.\n",
    "    :return: A dictionary representing the commit history or an error message.\n",
    "    \"\"\"\n",
    "    base_url = load_askthecode_api_base_url()\n",
    "    get_commit_history_url = f\"{base_url}/api/repository/commit/history\"\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'branch': branch,\n",
    "        'filePath': filePath\n",
    "    }\n",
    "    return make_api_request(get_commit_history_url, params, github_token)\n",
    "    \n",
    "# search repo code\n",
    "def search_repo_code(url: str, searchKeywords: List[str], branch: Optional[str] = None, relativePath: Optional[str] = None, searchHitLinesCount: Optional[int] = None, skipMatchesCount: Optional[int] = None) -> Union[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Searches for specific keywords within the code of a GitHub repository, with optional filtering by branch, path, and pagination controls.\n",
    "\n",
    "    :param url: The URL of the GitHub repository.\n",
    "    :param searchKeywords: A list of keywords to search within the repository code.\n",
    "    :param branch: Optional; the specific branch to search within.\n",
    "    :param relativePath: Optional; the specific directory path to limit the search to.\n",
    "    :param searchHitLinesCount: Optional; the number of lines to include in each search hit.\n",
    "    :param skipMatchesCount: Optional; the number of matches to skip (for pagination).\n",
    "    :return: A dictionary representing the search results or an error message.\n",
    "    \"\"\"\n",
    "    base_url = load_askthecode_api_base_url()\n",
    "    search_repo_code_url = f\"{base_url}/api/search/repository/code\"\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'searchKeywords': searchKeywords,\n",
    "        'branch': branch,\n",
    "        'relativePath': relativePath,\n",
    "        'searchHitLinesCount': searchHitLinesCount,\n",
    "        'skipMatchesCount': skipMatchesCount\n",
    "    }\n",
    "    return make_api_request(search_repo_code_url, params, github_token)\n",
    "\n",
    "# search repo commits\n",
    "def search_repo_commits(url: str, searchKeywords: List[str]) -> Union[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Searches for commits in a GitHub repository based on specified keywords.\n",
    "\n",
    "    :param url: The URL of the GitHub repository.\n",
    "    :param searchKeywords: A list of keywords to search for within the commit history.\n",
    "    :return: A dictionary representing the search results or an error message.\n",
    "    \"\"\"\n",
    "    base_url = load_askthecode_api_base_url()\n",
    "    search_repo_commits_url = f\"{base_url}/api/search/repository/commits\"\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'searchKeywords': searchKeywords\n",
    "    }\n",
    "    return make_api_request(search_repo_commits_url, params, github_token)\n",
    "    \n",
    "def find_repos(searchKeywords: List[str], language: Optional[str] = None) -> Union[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Searches for GitHub repositories based on specified keywords and optionally filtered by programming language.\n",
    "\n",
    "    :param searchKeywords: A list of keywords to search for repositories.\n",
    "    :param language: Optional; the programming language to filter the search results by.\n",
    "    :return: A dictionary representing the search results or an error message.\n",
    "    \"\"\"\n",
    "    base_url = load_askthecode_api_base_url()\n",
    "    find_repos_url = f\"{base_url}/api/search/repository\"\n",
    "    params = {\n",
    "        'searchKeywords': searchKeywords,\n",
    "        'language': language\n",
    "    }\n",
    "    return make_api_request(find_repos_url, params, github_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assistant API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'function', 'function': {'name': 'get_repo_structure', 'description': 'Retrieves the Github repository file structure to analyze it and be able to query only relevant files. If the provided URL contains specific branch and directory information, prioritize using that over querying the entire repository structure.', 'parameters': {'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}, 'branch': {'type': 'string', 'description': 'Repository branch. Provide only if user has explicitly specified it or the previous plugin response contains it.', 'nullable': True}, 'relativePaths': {'type': 'array', 'items': {'type': 'string'}, 'description': \"Relative paths to retrieve. USE only paths you are certain that exist, NEVER invent them. If the provided URL contains a specific directory path, extract and use it. Otherwise, this should be a directory path or pattern only. Patterns accept * symbol as 'any substring'\", 'nullable': True}}, 'required': ['url'], 'additionalProperties': False}}}, {'type': 'function', 'function': {'name': 'get_repo_content', 'description': 'Retrieves github repository file contents, possibly filtered by file names. Line numbers can be specified in URL as well. NEVER query this endpoint without previously querying get_repo_structure endpoint and when the next step is set to get_repo_structure.', 'parameters': {'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}, 'branch': {'type': 'string', 'description': 'Repository branch. Provide only if user has explicitly specified it or the previous assistant response contains it. When requesting file from commit, use commit SHA.', 'nullable': True}, 'relativePath': {'type': 'string', 'description': 'Relative paths to the directory. Provide only if user has explicitly specified it or the previous plugin response contains it.', 'nullable': True}, 'filePaths': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Files to query the content of. Order them by relevance descendant. This should NEVER contain the repository branch. First determine the branch if possible, and only then the file paths. Pass only if you are sure about the file path, call get_repo_structure otherwise'}}, 'required': ['url', 'filePaths'], 'additionalProperties': False}}}, {'type': 'function', 'function': {'name': 'get_repo_branches', 'description': 'Retrieves a list of branches from a Github repository given its URL.', 'parameters': {'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}}, 'required': ['url'], 'additionalProperties': False}}}, {'type': 'function', 'function': {'name': 'get_commit_history', 'description': 'Returns the commits history for the specific file in the repository. If the file path is not provided, the history of the entire repository will be returned. If the branch is not provided, the default branch will be used.', 'parameters': {'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}, 'branch': {'type': 'string', 'description': 'Repository branch. Provide only if user has explicitly specified it or the previous assistant response contains it.', 'nullable': True}, 'filePath': {'type': 'string', 'description': 'Path to the file to request the commit history for. Use path relative to the root directory of the repository.', 'nullable': True}}, 'required': ['url'], 'additionalProperties': False}}}, {'type': 'function', 'function': {'name': 'search_repo_code', 'description': 'Search code by user specified keywords. Use when user explicitly asked to search for something. Otherwise prefer to fetch the repository structure. Invoke only with user-specified, specific keywords (e.g., file, class, method names). Avoid generic terms.', 'parameters': {'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}, 'searchKeywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Search keywords. Invoke only with user-specified keywords. Never use keywords that are not part of the user prompt. When user asks to search for function definitions in a specific file (not directory) and you cannot parse them from file content, pass function keyword relevant for the file language..'}, 'branch': {'type': 'string', 'description': 'Repository branch. Provide only if user has explicitly specified it or the previous plugin response contains it. When requesting file from commit, use commit SHA.', 'nullable': True}, 'relativePath': {'type': 'string', 'description': 'Relative path to the file or directory to search in. Provide only if user has explicitly specified it or the previous plugin response contains it.', 'nullable': True}, 'searchHitLinesCount': {'type': 'integer', 'description': 'Number of lines to retrieve. Set only when explicitly asked to retrieve the specified amount of lines by the user.', 'format': 'int32', 'nullable': True}, 'skipMatchesCount': {'type': 'integer', 'description': 'Number of matches to skip in the file. use only when user is searching over file and you need to search for matches that were omitted from the previous search request', 'format': 'int32', 'nullable': True}}, 'required': ['url', 'searchKeywords'], 'additionalProperties': False}}}, {'type': 'function', 'function': {'name': 'search_repo_commits', 'description': 'Search commits by user specified keywords. Use only when user explicitly asked to search for commits and provided search query.', 'parameters': {'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}, 'searchKeywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Search keywords. Invoke only with user-specified keywords. Never use keywords that are not part of the user prompt.'}}, 'required': ['url', 'searchKeywords'], 'additionalProperties': False}}}, {'type': 'function', 'function': {'name': 'find_repos', 'description': 'Search repositories by user specified keywords. Use only when user explicitly asked to search for repositories and provided search query.', 'parameters': {'type': 'object', 'properties': {'searchKeywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Search keywords. Always use a single, specific keyword that best represents the topic. Avoid using multiple keywords for the same topic. OR logic applied, so providing multiple keywords for the same topic will worsen the results. Keywords should be singular, contain single word and clearly defined for precise searches.'}, 'language': {'type': 'string', 'description': 'Programming language. Use only when explicitly specified by the user.', 'nullable': True}}, 'required': ['searchKeywords'], 'additionalProperties': False}}}]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "def load_tools(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Loads tool definitions from a JSON file.\n",
    "\n",
    "    :param file_path: The path to the JSON file containing the tools definition.\n",
    "    :return: A list of tool definitions.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        tools = json.load(file)\n",
    "    return tools\n",
    "\n",
    "# use\n",
    "tools = load_tools('../tools.json')\n",
    "\n",
    "print(tools)\n",
    "print(type(tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_client(api_key: str) -> OpenAI:\n",
    "    \"\"\"\n",
    "    Initializes the OpenAI client with the given API key.\n",
    "\n",
    "    :param api_key: The API key for authenticating requests to OpenAI.\n",
    "    :return: An instance of the OpenAI client.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    return client\n",
    "\n",
    "from openai.types.beta.assistant import Assistant\n",
    "\n",
    "def create_assistant(client: OpenAI, name: str, instructions: str, model: str, tools: list) -> Assistant:\n",
    "    \"\"\"\n",
    "    Creates an assistant with the specified parameters using the provided OpenAI client.\n",
    "\n",
    "    :param client: The OpenAI client instance.\n",
    "    :param name: The name of the assistant.\n",
    "    :param instructions: The instructions for the assistant.\n",
    "    :param model: The model to be used by the assistant.\n",
    "    :param tools: The tools to be enabled for the assistant.\n",
    "    :return: The created assistant object.\n",
    "    \"\"\"\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=name,\n",
    "        instructions=instructions,\n",
    "        model=model,\n",
    "        tools=tools\n",
    "    )\n",
    "    #print(assistant)\n",
    "    return assistant\n",
    "\n",
    "def retrieve_assistant(client: OpenAI, assistant_id: str) -> Assistant:\n",
    "    \"\"\"\n",
    "    Retrieves an existing assistant by its ID using the provided OpenAI client.\n",
    "\n",
    "    :param client: The OpenAI client instance.\n",
    "    :param assistant_id: The ID of the assistant to retrieve.\n",
    "    :return: The retrieved assistant object.\n",
    "    \"\"\"\n",
    "    current_assistant = client.beta.assistants.retrieve(assistant_id)\n",
    "    #print(current_assistant)\n",
    "    return current_assistant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a message and run\n",
    "from typing import Tuple, Any, Union, Dict, Callable\n",
    "from openai.types.beta.assistant import Assistant\n",
    "from openai.types.beta.thread import Thread\n",
    "from openai.types.beta.threads.run import Run\n",
    "\n",
    "\n",
    "def create_message_and_run(assistant: Assistant, query: str, thread: Thread = None) -> Tuple[Run, Thread]:\n",
    "    \"\"\"\n",
    "    Creates a message and initiates a run for a given query within a thread. If no thread is provided, a new one is created.\n",
    "\n",
    "    :param assistant: The Assistant object to use for creating the run.\n",
    "    :param query: The user's query to send to the assistant.\n",
    "    :param thread: Optional; The thread object to continue the conversation. A new thread is created if not provided.\n",
    "    :return: A tuple containing the Run object that was created and the Thread object used or created.\n",
    "    \"\"\"\n",
    "    # Create a new thread if not provided\n",
    "    if not thread: \n",
    "        thread = client.beta.threads.create()\n",
    "\n",
    "    # Create a message\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=query\n",
    "    )\n",
    "    # Create a run\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id\n",
    "    )\n",
    "    return run, thread\n",
    "\n",
    "# Utility function to get details of function to be called\n",
    "\n",
    "def get_function_details(run: Run) -> Tuple[str, Any, str]:\n",
    "    \"\"\"\n",
    "    Extracts and prints details about the function to be called based on the run's required action.\n",
    "\n",
    "    :param run: The Run object containing details about the required action.\n",
    "    :return: A tuple containing the function name to be called, its arguments, and the function call's ID.\n",
    "    \"\"\"\n",
    "    print(\"\\nRun Action Required\")  # Placeholder for actual run.required_action\n",
    "\n",
    "    function_name = run.required_action.submit_tool_outputs.tool_calls[0].function.name\n",
    "    arguments = run.required_action.submit_tool_outputs.tool_calls[0].function.arguments\n",
    "    function_id = run.required_action.submit_tool_outputs.tool_calls[0].id\n",
    "\n",
    "    print(f\"Function Called: {function_name} with arguments: {arguments}\")\n",
    "\n",
    "    return function_name, arguments, function_id\n",
    "\n",
    "# Utility function to submit the function response\n",
    "\n",
    "def submit_tool_outputs(run: Run, thread: Thread, function_id: str, function_response: Any) -> Run:\n",
    "    \"\"\"\n",
    "    Submits the output of a tool function call as part of the conversation with an assistant.\n",
    "\n",
    "    :param run: The Run object representing the current assistant's run.\n",
    "    :param thread: The Thread object where the conversation is taking place.\n",
    "    :param function_id: The identifier of the function call within the assistant's run.\n",
    "    :param function_response: The response from the function call to be submitted.\n",
    "    :return: An updated Run object after submitting the tool outputs.\n",
    "    \"\"\"\n",
    "    run = client.beta.threads.runs.submit_tool_outputs(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id,\n",
    "        tool_outputs=[\n",
    "            {\n",
    "                \"tool_call_id\": function_id,\n",
    "                \"output\": str(function_response),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return run\n",
    "\n",
    "def create_function_executor() -> Dict[str, Callable]:\n",
    "    \"\"\"\n",
    "    Creates and returns a dictionary of available functions that can be executed \n",
    "    by the assistant.\n",
    "\n",
    "    :return: A dictionary with function names as keys and callable Python functions as values.\n",
    "    \"\"\"\n",
    "    available_functions = {\n",
    "        \"get_repo_structure\": get_repo_structure,\n",
    "        \"get_repo_content\": get_repo_content,\n",
    "        \"get_repo_branches\": get_repo_branches,\n",
    "        \"get_commit_history\": get_commit_history,\n",
    "        \"search_repo_code\": search_repo_code,\n",
    "        \"search_repo_commits\": search_repo_commits,\n",
    "        \"find_repos\": find_repos\n",
    "    }\n",
    "    return available_functions\n",
    "\n",
    "def execute_function_call(function_name: str, arguments: str) -> Union[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Executes a named function with provided JSON-formatted string arguments.\n",
    "\n",
    "    :param function_name: The name of the function to be executed.\n",
    "    :param arguments: A JSON-formatted string representing the arguments for the function.\n",
    "    :return: The result of the function execution, which could be a dictionary or an error message.\n",
    "    \"\"\"\n",
    "    available_functions = create_function_executor()\n",
    "    function = available_functions.get(function_name, None)\n",
    "    if function:\n",
    "        arguments = json.loads(arguments)\n",
    "        results = function(**arguments)\n",
    "    else:\n",
    "        results = f\"Error: function {function_name} does not exist\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_thread(client: OpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Creates a new thread using the OpenAI client.\n",
    "\n",
    "    :param client: The OpenAI client instance.\n",
    "    :return: The ID of the newly created thread.\n",
    "    \"\"\"\n",
    "    thread = client.beta.threads.create()\n",
    "    print(f\"New thread created with ID: {thread.id}\")\n",
    "    return thread.id\n",
    "\n",
    "def load_thread(client: OpenAI, thread_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads an existing thread by its ID.\n",
    "\n",
    "    :param client: The OpenAI client instance.\n",
    "    :param thread_id: The ID of the thread to load.\n",
    "    :return: The thread object.\n",
    "    \"\"\"\n",
    "    thread = client.beta.threads.retrieve(thread_id)\n",
    "    print(f\"Thread {thread_id} loaded successfully.\")\n",
    "    return thread\n",
    "\n",
    "def run_chatbot(client: OpenAI, assistant: Assistant, thread_id: str = None):\n",
    "    \"\"\"\n",
    "    Runs the main chatbot loop, allowing user interaction with the assistant.\n",
    "\n",
    "    :param client: The OpenAI client instance.\n",
    "    :param assistant: The Assistant object to use.\n",
    "    :param thread_id: Optional; the ID of an existing thread to continue the conversation from.\n",
    "    \"\"\"\n",
    "    if thread_id:\n",
    "        # Load an existing thread if an ID is provided\n",
    "        thread = load_thread(client, thread_id)\n",
    "    else:\n",
    "        # Create a new thread otherwise\n",
    "        thread_id = create_new_thread(client)\n",
    "        thread = load_thread(client, thread_id)  # Load the newly created thread for consistency\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Please enter your query or type 'STOP' to exit: \")\n",
    "        if user_input.lower() == \"stop\":\n",
    "            break\n",
    "\n",
    "        run, _ = create_message_and_run(assistant=assistant, query=user_input, thread=thread)\n",
    "\n",
    "        while True:\n",
    "            run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "            print(\"run status\", run.status)\n",
    "\n",
    "            # Check if the run requires action and execute the function call if so\n",
    "            if run.status == \"requires_action\":\n",
    "                function_name, arguments, function_id = get_function_details(run)\n",
    "                function_response = execute_function_call(function_name, arguments)\n",
    "                run = submit_tool_outputs(run, thread, function_id, function_response)\n",
    "                continue\n",
    "            \n",
    "            # Check if the run is completed and display the assistant's response\n",
    "            if run.status == \"completed\":\n",
    "                messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "                latest_message = messages.data[0]\n",
    "                text = latest_message.content[0].text.value\n",
    "                print(f'User: {user_input}')\n",
    "                print(f'Assistant: {text}')\n",
    "                break\n",
    "\n",
    "            time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key loaded successfully.\n",
      "GitHub token loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New thread created with ID: thread_CLUR4ofBs6k9WKAvJd4t91eX\n",
      "Thread thread_CLUR4ofBs6k9WKAvJd4t91eX loaded successfully.\n",
      "run status in_progress\n",
      "run status requires_action\n",
      "\n",
      "Run Action Required\n",
      "Function Called: get_repo_structure with arguments: {\"url\":\"https://github.com/RecandChat/CodeCompass/tree/main\"}\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status requires_action\n",
      "\n",
      "Run Action Required\n",
      "Function Called: get_repo_content with arguments: {\"url\":\"https://github.com/RecandChat/CodeCompass/tree/main\",\"filePaths\":[\"app.py\",\"codecompasslib/eda.ipynb\",\"codecompasslib/models/cosine_modelling_repos.ipynb\",\"codecompasslib/models/model_diff_repos.py\",\"codecompasslib/models/modelling_differentiating_repos.ipynb\",\"codecompasslib/test_drive.ipynb\",\"backend/__init__.py\",\"frontend/search.html\",\"tests/test_drive.py\",\"tests/test_functional.py\",\"docs/README.md\",\"LICENSE\",\"requirements.txt\",\".github/workflows/pr_gate.yml\",\"allReposCleaned.csv\",\"codecompasslib/clean_data.py\",\"codecompasslib/API/drive_operations.py\",\"codecompasslib/API/get_bulk_data.py\",\"codecompasslib/API/helper_functions.py\",\"codecompasslib/Data/original/bulkData.csv\",\"codecompasslib/Data/original/mostFamousUsersRepos.csv\",\"codecompasslib/__init__.py\",\"tests/conftest.py\"]}\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status completed\n",
      "User: Tell me about the following repository: https://github.com/RecandChat/CodeCompass/tree/main\n",
      "Assistant: I have retrieved the contents of some relevant files from the \"CodeCompass\" repository. Here is a summary of the content for each file:\n",
      "\n",
      "1. **app.py**:\n",
      "   ```python\n",
      "   from backend import app\n",
      "\n",
      "   if __name__ == '__main__':\n",
      "       app.run(debug=True, port=5000)\n",
      "   ```\n",
      "\n",
      "2. **eda.ipynb**: An exploratory data analysis notebook for the repository data.\n",
      "\n",
      "3. **cosine_modelling_repos.ipynb**: A model using cosine similarities to recommend repositories based on user preferences.\n",
      "\n",
      "4. **model_diff_repos.py**: Python script for differentiating repositories.\n",
      "\n",
      "5. **modelling_differentiating_repos.ipynb**: A model used to recommend GitHub repositories based on differentiating experience.\n",
      "\n",
      "6. **test_drive.ipynb**: A notebook for testing the drive operations.\n",
      "\n",
      "7. **backend/__init__.py**: Initialization file for the backend.\n",
      "\n",
      "8. **frontend/search.html**: HTML file for the frontend search functionality.\n",
      "\n",
      "9. **tests/test_drive.py**: Test script for the drive operations.\n",
      "\n",
      "10. **tests/test_functional.py**: Functional test script.\n",
      "\n",
      "11. **docs/README.md**: README file for the repository.\n",
      "\n",
      "12. **LICENSE**: License file for the repository.\n",
      "\n",
      "13. **requirements.txt**: File listing the required dependencies.\n",
      "\n",
      "14. **.github/workflows/pr_gate.yml**: GitHub Actions workflow for pull request gating.\n",
      "\n",
      "15. **allReposCleaned.csv**: CSV file containing cleaned repository data.\n",
      "\n",
      "16. **codecompasslib/clean_data.py**: Python script for cleaning data.\n",
      "\n",
      "17. **codecompasslib/API/drive_operations.py**: API file for drive operations.\n",
      "\n",
      "18. **codecompasslib/API/get_bulk_data.py**: API file for getting bulk data.\n",
      "\n",
      "19. **codecompasslib/API/helper_functions.py**: API file containing helper functions.\n",
      "\n",
      "20. **codecompasslib/Data/original/bulkData.csv**: Original bulk data CSV file.\n",
      "\n",
      "21. **codecompasslib/Data/original/mostFamousUsersRepos.csv**: CSV file containing data of the most famous users repositories.\n",
      "\n",
      "22. **codecompasslib/__init__.py**: Initialization file for the code compass library.\n",
      "\n",
      "23. **tests/conftest.py**: Configuration file for tests.\n",
      "\n",
      "Would you like more details on any specific file or have any other questions related to the repository?\n"
     ]
    }
   ],
   "source": [
    "# Full Usage:\n",
    "\n",
    "# Load secrets\n",
    "openAI_key = load_openai_key()\n",
    "github_token = load_github_token()\n",
    "\n",
    "# Initialize client\n",
    "client = initialize_client(openAI_key)\n",
    "\n",
    "# Create assistant or retrieve existing assistant\n",
    "# Useage\n",
    "client = initialize_client(openAI_key)\n",
    "new_assistant = create_assistant(\n",
    "    client=client,\n",
    "    name=\"CodeCompass\",\n",
    "    instructions=\"You are a helpful assistant that analyzes code from github repositories and files when given a github url. You will answer questions about the structure of a repository, the content of files, or any other code-related queries.\",\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    tools=load_tools('../tools.json')\n",
    ")\n",
    "\n",
    "# Retrieve the assistant\n",
    "older_assistant = retrieve_assistant(client, 'asst_gDv1PyXxMQ0GPQLYyh5KUm9C')\n",
    "\n",
    "# Run the chatbot\n",
    "run_chatbot(client, new_assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the structure of the repository:\n",
      "{'branchName': 'main', 'files': ['.devcontainer/Dockerfile', '.devcontainer/devcontainer.json', '.github/.codecov.yml', '.github/CODEOWNERS', '.github/ISSUE_TEMPLATE.md', '.github/ISSUE_TEMPLATE/bug_report.md', '.github/ISSUE_TEMPLATE/feature_request.md', '.github/ISSUE_TEMPLATE/general-ask.md', '.github/PULL_REQUEST_TEMPLATE.md', '.github/actions/azureml-test/action.yml', '.github/actions/get-test-groups/action.yml', '.github/workflows/azureml-cpu-nightly.yml', '.github/workflows/azureml-gpu-nightly.yml', '.github/workflows/azureml-release-pipeline.yml', '.github/workflows/azureml-spark-nightly.yml', '.github/workflows/azureml-unit-tests.yml', '.github/workflows/sarplus.yml', '.github/workflows/update_documentation.yml', 'AUTHORS.md', 'CODE_OF_CONDUCT.md', 'CONTRIBUTING.md', 'GLOSSARY.md', 'LICENSE', 'MANIFEST.in', 'NEWS.md', 'README.md', 'SECURITY.md', 'SETUP.md', 'contrib/README.md', 'contrib/azureml_designer_modules/README.md', 'contrib/azureml_designer_modules/entries/map_entry.py', 'contrib/azureml_designer_modules/entries/ndcg_entry.py', 'contrib/azureml_designer_modules/entries/precision_at_k_entry.py', 'contrib/azureml_designer_modules/entries/recall_at_k_entry.py', 'contrib/azureml_designer_modules/entries/score_sar_entry.py', 'contrib/azureml_designer_modules/entries/stratified_splitter_entry.py', 'contrib/azureml_designer_modules/entries/train_sar_entry.py', 'contrib/azureml_designer_modules/module_specs/map.yaml', 'contrib/azureml_designer_modules/module_specs/ndcg.yaml', 'contrib/azureml_designer_modules/module_specs/precision_at_k.yaml', 'contrib/azureml_designer_modules/module_specs/recall_at_k.yaml', 'contrib/azureml_designer_modules/module_specs/sar_conda.yaml', 'contrib/azureml_designer_modules/module_specs/sar_score.yaml', 'contrib/azureml_designer_modules/module_specs/sar_train.yaml', 'contrib/azureml_designer_modules/module_specs/stratified_splitter.yaml', 'contrib/sarplus/DEVELOPMENT.md', 'contrib/sarplus/README.md', 'contrib/sarplus/VERSION', 'contrib/sarplus/python/.flake8', 'contrib/sarplus/python/README.md', 'contrib/sarplus/python/pyproject.toml', 'contrib/sarplus/python/pysarplus/SARModel.py', 'contrib/sarplus/python/pysarplus/SARPlus.py', 'contrib/sarplus/python/pysarplus/__init__.py', 'contrib/sarplus/python/setup.py', 'contrib/sarplus/python/src/pysarplus.cpp', 'contrib/sarplus/python/tests/conftest.py', 'contrib/sarplus/python/tests/sample-input.txt', 'contrib/sarplus/python/tests/test_pyspark_sar.py', 'contrib/sarplus/scala/build.sbt', 'contrib/sarplus/scala/compat/src/main/scala/com/microsoft/sarplus/compat/spark/since3p2defvisible.scala', 'contrib/sarplus/scala/project/Utils.scala', 'contrib/sarplus/scala/project/build.properties', 'contrib/sarplus/scala/project/plugins.sbt', 'contrib/sarplus/scala/python/pysarplus_dummy/__init__.py', 'contrib/sarplus/scala/python/setup.py', 'contrib/sarplus/scala/src/main/scala/com/microsoft/sarplus/DefaultSource.scala', 'contrib/sarplus/scala/src/main/scala/com/microsoft/sarplus/SARCacheOutputWriter.scala', 'contrib/sarplus/scala/src/main/scala/com/microsoft/sarplus/SARCacheOutputWriterFactory.scala', 'contrib/sarplus/scala/src/test/scala/com/microsoft/sarplus/SARCacheOutputWriterSpec.scala', 'docs/_config.yml', 'docs/_toc.yml', 'docs/datasets.rst', 'docs/evaluation.rst', 'docs/intro.md', 'docs/models.rst', 'docs/requirements-doc.txt', 'docs/tuning.rst', 'docs/utils.rst', 'examples/00_quick_start/README.md', 'examples/00_quick_start/als_movielens.ipynb', 'examples/00_quick_start/dkn_MIND.ipynb', 'examples/00_quick_start/fastai_movielens.ipynb', 'examples/00_quick_start/geoimc_movielens.ipynb', 'examples/00_quick_start/lightgbm_tinycriteo.ipynb', 'examples/00_quick_start/lstur_MIND.ipynb', 'examples/00_quick_start/naml_MIND.ipynb', 'examples/00_quick_start/ncf_movielens.ipynb', 'examples/00_quick_start/npa_MIND.ipynb', 'examples/00_quick_start/nrms_MIND.ipynb', 'examples/00_quick_start/rbm_movielens.ipynb', 'examples/00_quick_start/rlrmc_movielens.ipynb', 'examples/00_quick_start/sar_movielens.ipynb', 'examples/00_quick_start/sar_movielens_with_azureml.ipynb', 'examples/00_quick_start/sar_movieratings_with_azureml_designer.ipynb', 'examples/00_quick_start/sasrec_amazon.ipynb', 'examples/00_quick_start/sequential_recsys_amazondataset.ipynb', 'examples/00_quick_start/tfidf_covid.ipynb', 'examples/00_quick_start/wide_deep_movielens.ipynb', 'examples/00_quick_start/xdeepfm_criteo.ipynb', 'examples/01_prepare_data/README.md', 'examples/01_prepare_data/data_split.ipynb', 'examples/01_prepare_data/data_transform.ipynb', 'examples/01_prepare_data/mind_utils.ipynb', 'examples/01_prepare_data/wikidata_knowledge_graph.ipynb', 'examples/02_model_collaborative_filtering/README.md', 'examples/02_model_collaborative_filtering/als_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/baseline_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/fm_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/lightfm_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/rbm_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/sar_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb', 'examples/02_model_content_based_filtering/README.md', 'examples/02_model_content_based_filtering/dkn_deep_dive.ipynb', 'examples/02_model_content_based_filtering/mmlspark_lightgbm_criteo.ipynb', 'examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb', 'examples/03_evaluate/README.md', 'examples/03_evaluate/als_movielens_diversity_metrics.ipynb', 'examples/03_evaluate/evaluation.ipynb', 'examples/04_model_select_and_optimize/README.md', 'examples/04_model_select_and_optimize/azureml_hyperdrive_surprise_svd.ipynb', 'examples/04_model_select_and_optimize/azureml_hyperdrive_wide_and_deep.ipynb', 'examples/04_model_select_and_optimize/nni_ncf.ipynb', 'examples/04_model_select_and_optimize/nni_surprise_svd.ipynb', 'examples/04_model_select_and_optimize/train_scripts/svd_training.py', 'examples/04_model_select_and_optimize/train_scripts/wide_deep_training.py', 'examples/04_model_select_and_optimize/tuning_spark_als.ipynb', 'examples/05_operationalize/README.md', 'examples/05_operationalize/aks_locust_load_test.ipynb', 'examples/05_operationalize/als_movie_o16n.ipynb', 'examples/05_operationalize/lightgbm_criteo_o16n.ipynb', 'examples/06_benchmarks/README.md', 'examples/06_benchmarks/benchmark_utils.py', 'examples/06_benchmarks/movielens.ipynb', 'examples/07_tutorials/KDD2020-tutorial/README.md', 'examples/07_tutorials/KDD2020-tutorial/dkn.yaml', 'examples/07_tutorials/KDD2020-tutorial/lightgcn.yaml', 'examples/07_tutorials/KDD2020-tutorial/pandas-subgraph-local-samples.ipynb', 'examples/07_tutorials/KDD2020-tutorial/reco_cpu_kdd.yaml', 'examples/07_tutorials/KDD2020-tutorial/reco_gpu_kdd.yaml', 'examples/07_tutorials/KDD2020-tutorial/run_transE.sh', 'examples/07_tutorials/KDD2020-tutorial/step1_data_preparation.ipynb', 'examples/07_tutorials/KDD2020-tutorial/step2_pretraining-embeddings.ipynb', 'examples/07_tutorials/KDD2020-tutorial/step3_run_dkn.ipynb', 'examples/07_tutorials/KDD2020-tutorial/step4_run_dkn_item2item.ipynb', 'examples/07_tutorials/KDD2020-tutorial/step5_run_lightgcn.ipynb', 'examples/07_tutorials/KDD2020-tutorial/utils/PandasMagClass.py', 'examples/07_tutorials/KDD2020-tutorial/utils/data_helper.py', 'examples/07_tutorials/KDD2020-tutorial/utils/general.py', 'examples/07_tutorials/KDD2020-tutorial/utils/task_helper.py', 'examples/README.md', 'examples/run_notebook_on_azureml.ipynb', 'examples/template.ipynb', 'pyproject.toml', 'recommenders/README.md', 'recommenders/__init__.py', 'recommenders/datasets/__init__.py', 'recommenders/datasets/amazon_reviews.py', 'recommenders/datasets/cosmos_cli.py', 'recommenders/datasets/covid_utils.py', 'recommenders/datasets/criteo.py', 'recommenders/datasets/download_utils.py', 'recommenders/datasets/mind.py', 'recommenders/datasets/movielens.py', 'recommenders/datasets/pandas_df_utils.py', 'recommenders/datasets/python_splitters.py', 'recommenders/datasets/spark_splitters.py', 'recommenders/datasets/sparse.py', 'recommenders/datasets/split_utils.py', 'recommenders/datasets/wikidata.py', 'recommenders/evaluation/__init__.py', 'recommenders/evaluation/python_evaluation.py', 'recommenders/evaluation/spark_evaluation.py', 'recommenders/models/__init__.py', 'recommenders/models/cornac/__init__.py', 'recommenders/models/cornac/cornac_utils.py', 'recommenders/models/deeprec/DataModel/ImplicitCF.py', 'recommenders/models/deeprec/DataModel/__init__.py', 'recommenders/models/deeprec/__init__.py', 'recommenders/models/deeprec/config/asvd.yaml', 'recommenders/models/deeprec/config/caser.yaml', 'recommenders/models/deeprec/config/gru.yaml', 'recommenders/models/deeprec/config/lightgcn.yaml', 'recommenders/models/deeprec/config/nextitnet.yaml', 'recommenders/models/deeprec/config/sli_rec.yaml', 'recommenders/models/deeprec/config/sum.yaml', 'recommenders/models/deeprec/deeprec_utils.py', 'recommenders/models/deeprec/io/__init__.py', 'recommenders/models/deeprec/io/dkn_item2item_iterator.py', 'recommenders/models/deeprec/io/dkn_iterator.py', 'recommenders/models/deeprec/io/iterator.py', 'recommenders/models/deeprec/io/nextitnet_iterator.py', 'recommenders/models/deeprec/io/sequential_iterator.py', 'recommenders/models/deeprec/models/__init__.py', 'recommenders/models/deeprec/models/base_model.py', 'recommenders/models/deeprec/models/dkn.py', 'recommenders/models/deeprec/models/dkn_item2item.py', 'recommenders/models/deeprec/models/graphrec/__init__.py', 'recommenders/models/deeprec/models/graphrec/lightgcn.py', 'recommenders/models/deeprec/models/sequential/__init__.py', 'recommenders/models/deeprec/models/sequential/asvd.py', 'recommenders/models/deeprec/models/sequential/caser.py', 'recommenders/models/deeprec/models/sequential/gru.py', 'recommenders/models/deeprec/models/sequential/nextitnet.py', 'recommenders/models/deeprec/models/sequential/rnn_cell_implement.py', 'recommenders/models/deeprec/models/sequential/sequential_base_model.py', 'recommenders/models/deeprec/models/sequential/sli_rec.py', 'recommenders/models/deeprec/models/sequential/sum.py', 'recommenders/models/deeprec/models/sequential/sum_cells.py', 'recommenders/models/deeprec/models/xDeepFM.py', 'recommenders/models/fastai/__init__.py', 'recommenders/models/fastai/fastai_utils.py', 'recommenders/models/geoimc/__init__.py', 'recommenders/models/geoimc/geoimc_algorithm.py', 'recommenders/models/geoimc/geoimc_data.py', 'recommenders/models/geoimc/geoimc_predict.py', 'recommenders/models/geoimc/geoimc_utils.py', 'recommenders/models/lightfm/__init__.py', 'recommenders/models/lightfm/lightfm_utils.py', 'recommenders/models/lightgbm/__init__.py', 'recommenders/models/lightgbm/lightgbm_utils.py', 'recommenders/models/ncf/__init__.py', 'recommenders/models/ncf/dataset.py', 'recommenders/models/ncf/ncf_singlenode.py', 'recommenders/models/newsrec/__init__.py', 'recommenders/models/newsrec/io/__init__.py', 'recommenders/models/newsrec/io/mind_all_iterator.py', 'recommenders/models/newsrec/io/mind_iterator.py', 'recommenders/models/newsrec/models/__init__.py', 'recommenders/models/newsrec/models/base_model.py', 'recommenders/models/newsrec/models/layers.py', 'recommenders/models/newsrec/models/lstur.py', 'recommenders/models/newsrec/models/naml.py', 'recommenders/models/newsrec/models/npa.py', 'recommenders/models/newsrec/models/nrms.py', 'recommenders/models/newsrec/newsrec_utils.py', 'recommenders/models/rbm/__init__.py', 'recommenders/models/rbm/rbm.py', 'recommenders/models/rlrmc/RLRMCalgorithm.py', 'recommenders/models/rlrmc/RLRMCdataset.py', 'recommenders/models/rlrmc/__init__.py', 'recommenders/models/rlrmc/conjugate_gradient_ms.py', 'recommenders/models/sar/__init__.py', 'recommenders/models/sar/sar_singlenode.py', 'recommenders/models/sasrec/__init__.py', 'recommenders/models/sasrec/model.py', 'recommenders/models/sasrec/sampler.py', 'recommenders/models/sasrec/ssept.py', 'recommenders/models/sasrec/util.py', 'recommenders/models/surprise/__init__.py', 'recommenders/models/surprise/surprise_utils.py', 'recommenders/models/tfidf/__init__.py', 'recommenders/models/tfidf/tfidf_utils.py', 'recommenders/models/vae/__init__.py', 'recommenders/models/vae/multinomial_vae.py', 'recommenders/models/vae/standard_vae.py', 'recommenders/models/vowpal_wabbit/__init__.py', 'recommenders/models/vowpal_wabbit/vw.py', 'recommenders/models/wide_deep/__init__.py', 'recommenders/models/wide_deep/wide_deep_utils.py', 'recommenders/tuning/__init__.py', 'recommenders/tuning/nni/__init__.py', 'recommenders/tuning/nni/ncf_training.py', 'recommenders/tuning/nni/ncf_utils.py', 'recommenders/tuning/nni/nni_utils.py', 'recommenders/tuning/nni/svd_training.py', 'recommenders/tuning/parameter_sweep.py', 'recommenders/utils/__init__.py', 'recommenders/utils/constants.py', 'recommenders/utils/general_utils.py', 'recommenders/utils/gpu_utils.py', 'recommenders/utils/k8s_utils.py', 'recommenders/utils/notebook_memory_management.py', 'recommenders/utils/notebook_utils.py', 'recommenders/utils/plot.py', 'recommenders/utils/python_utils.py', 'recommenders/utils/spark_utils.py', 'recommenders/utils/tf_utils.py', 'recommenders/utils/timer.py', 'scenarios/README.md', 'scenarios/ads/README.md', 'scenarios/food_and_restaurants/README.md', 'scenarios/gaming/README.md', 'scenarios/news/README.md', 'scenarios/retail/README.md', 'scenarios/travel/README.md', 'setup.py', 'tests/README.md', 'tests/__init__.py', 'tests/ci/README.md', 'tests/ci/__init__.py', 'tests/ci/azureml_tests/__init__.py', 'tests/ci/azureml_tests/run_groupwise_pytest.py', 'tests/ci/azureml_tests/submit_groupwise_azureml_pytest.py', 'tests/ci/azureml_tests/test_groups.py', 'tests/conftest.py', 'tests/data_validation/__init__.py', 'tests/data_validation/examples/__init__.py', 'tests/data_validation/examples/test_mind.py', 'tests/data_validation/examples/test_wikidata.py', 'tests/data_validation/recommenders/__init__.py', 'tests/data_validation/recommenders/datasets/__init__.py', 'tests/data_validation/recommenders/datasets/test_covid_utils.py', 'tests/data_validation/recommenders/datasets/test_criteo.py', 'tests/data_validation/recommenders/datasets/test_mind.py', 'tests/data_validation/recommenders/datasets/test_movielens.py', 'tests/data_validation/recommenders/datasets/test_wikidata.py', 'tests/functional/__init__.py', 'tests/functional/examples/__init__.py', 'tests/functional/examples/test_notebooks_gpu.py', 'tests/functional/examples/test_notebooks_pyspark.py', 'tests/functional/examples/test_notebooks_python.py', 'tests/integration/__init__.py', 'tests/integration/recommenders/__init__.py', 'tests/integration/recommenders/utils/__init__.py', 'tests/integration/recommenders/utils/test_k8s_utils.py', 'tests/performance/__init__.py', 'tests/performance/recommenders/__init__.py', 'tests/performance/recommenders/evaluation/__init__.py', 'tests/performance/recommenders/evaluation/test_python_evaluation_time_performance.py', 'tests/regression/__init__.py', 'tests/regression/test_compatibility_tf.py', 'tests/responsible_ai/__init__.py', 'tests/responsible_ai/recommenders/__init__.py', 'tests/responsible_ai/recommenders/datasets/__init__.py', 'tests/responsible_ai/recommenders/datasets/test_criteo_privacy.py', 'tests/responsible_ai/recommenders/datasets/test_movielens_privacy.py', 'tests/security/__init__.py', 'tests/security/test_dependency_security.py', 'tests/smoke/__init__.py', 'tests/smoke/examples/__init__.py', 'tests/smoke/examples/test_notebooks_gpu.py', 'tests/smoke/examples/test_notebooks_pyspark.py', 'tests/smoke/examples/test_notebooks_python.py', 'tests/smoke/recommenders/__init__.py', 'tests/smoke/recommenders/recommender/__init__.py', 'tests/smoke/recommenders/recommender/test_deeprec_model.py', 'tests/smoke/recommenders/recommender/test_deeprec_utils.py', 'tests/smoke/recommenders/recommender/test_newsrec_model.py', 'tests/smoke/recommenders/recommender/test_newsrec_utils.py', 'tests/unit/__init__.py', 'tests/unit/examples/__init__.py', 'tests/unit/examples/test_notebooks_gpu.py', 'tests/unit/examples/test_notebooks_pyspark.py', 'tests/unit/examples/test_notebooks_python.py', 'tests/unit/recommenders/__init__.py', 'tests/unit/recommenders/datasets/__init__.py', 'tests/unit/recommenders/datasets/test_download_utils.py', 'tests/unit/recommenders/datasets/test_pandas_df_utils.py', 'tests/unit/recommenders/datasets/test_python_splitter.py', 'tests/unit/recommenders/datasets/test_spark_splitter.py', 'tests/unit/recommenders/datasets/test_sparse.py', 'tests/unit/recommenders/evaluation/__init__.py', 'tests/unit/recommenders/evaluation/conftest.py', 'tests/unit/recommenders/evaluation/test_python_evaluation.py', 'tests/unit/recommenders/evaluation/test_spark_evaluation.py', 'tests/unit/recommenders/models/__init__.py', 'tests/unit/recommenders/models/test_cornac_utils.py', 'tests/unit/recommenders/models/test_deeprec_model.py', 'tests/unit/recommenders/models/test_deeprec_utils.py', 'tests/unit/recommenders/models/test_geoimc.py', 'tests/unit/recommenders/models/test_lightfm_utils.py', 'tests/unit/recommenders/models/test_ncf_dataset.py', 'tests/unit/recommenders/models/test_ncf_singlenode.py', 'tests/unit/recommenders/models/test_newsrec_model.py', 'tests/unit/recommenders/models/test_newsrec_utils.py', 'tests/unit/recommenders/models/test_rbm.py', 'tests/unit/recommenders/models/test_sar_singlenode.py', 'tests/unit/recommenders/models/test_sasrec_model.py', 'tests/unit/recommenders/models/test_surprise_utils.py', 'tests/unit/recommenders/models/test_tfidf_utils.py', 'tests/unit/recommenders/models/test_vowpal_wabbit.py', 'tests/unit/recommenders/models/test_wide_deep_utils.py', 'tests/unit/recommenders/tuning/__init__.py', 'tests/unit/recommenders/tuning/test_ncf_utils.py', 'tests/unit/recommenders/tuning/test_nni_utils.py', 'tests/unit/recommenders/tuning/test_sweep.py', 'tests/unit/recommenders/utils/__init__.py', 'tests/unit/recommenders/utils/programmatic_notebook_execution.ipynb', 'tests/unit/recommenders/utils/test_general_utils.py', 'tests/unit/recommenders/utils/test_gpu_utils.py', 'tests/unit/recommenders/utils/test_notebook_utils.ipynb', 'tests/unit/recommenders/utils/test_notebook_utils.py', 'tests/unit/recommenders/utils/test_plot.py', 'tests/unit/recommenders/utils/test_python_utils.py', 'tests/unit/recommenders/utils/test_tf_utils.py', 'tests/unit/recommenders/utils/test_timer.py', 'tools/__init__.py', 'tools/databricks_install.py', 'tools/docker/Dockerfile', 'tools/docker/README.md'], 'assistantNextSteps': '1. Thoroughly analyze the repository structure and try to make some assumptions on the content of each file.2. Request contents of no more than 20 files. Order them by relevance descending. '}\n",
      "Here is the content of the repository:\n",
      "{'branchName': 'main', 'files': [{'path': '.github/.codecov.yml', 'content': '# This file controls how codecov submit comments in the PR about code coverage.\\n# For more details, please see: \\n# https://docs.codecov.com/docs/pull-request-comments#section-behavior\\ncomment:\\n  behavior: default\\n\\nflags:\\n  nightly:\\n    joined: false'}], 'assistantNextSteps': '1. Analyze if the answer to the user question is contained in the returned files. 2. If yes, print the response to the user. If not, try to request the contents of another files.'}\n",
      "Here are the branches of the repository:\n",
      "{'branches': [{'name': 'chatbot', 'headSha': 'aae146df9942c6dabaa059f259b074ee5023632f'}, {'name': 'llm-based-embeddings', 'headSha': '2a50ee5c11db2ccf46917cfeaf5e767ca97054c0'}, {'name': 'main', 'headSha': '87dcdbf196b78cf208b80a79973a9d69d763dee6'}, {'name': 'mlops', 'headSha': 'a194bec38218b170253532e9ca76f5b770f27937'}], 'assistantNextSteps': \"1. Respond with the list of branches. If not specified by user, render them as the list, where you'll display the branch name and the SHA of last commit. 2. Say to user that he can request the commit details for the last commit of each branch.\"}\n",
      "Here is the commit history of the repository:\n",
      "{'filePath': '.devcontainer/devcontainer.json', 'commits': [{'commitUrl': 'https://github.com/recommenders-team/recommenders/commit/846d214476bfb1f4e6898503bebb6a33a14db410', 'message': 'Adding codespace deployment (#1521)\\n\\n* adding codespace configuration\\r\\n\\r\\n* adding codespace configuration and dockerfile\\r\\n\\r\\n* fix java installation in codespace docker\\r\\n\\r\\n* adding installation for  package option\\r\\n\\r\\n* updating packages installed\\r\\n\\r\\n* updating ipykernel version and port forwarded for codespace\\r\\n\\r\\n* updating pip install and path\\r\\n\\r\\n* reverting postcreatecommand changes\\r\\n\\r\\n* using root for codespace\\r\\n\\r\\n* updating jupyter notebook setup and trying non-root user\\r\\n\\r\\n* fixes for using non-root user\\r\\n\\r\\n* allowing editable install into user by disabling pep517 for pip\\r\\n\\r\\n* allow user editable pip installs with user flag\\r\\n\\r\\n* removing duplicate env\\r\\n\\r\\n* adding port 4040 for spark monitoring\\r\\n\\r\\n* removing 4040 port fwd', 'commitDate': '2021-10-08T16:52:51', 'author': 'gramhagen'}], 'count': 1, 'assistantNextSteps': \"1. Print that you've successfully retrieved 1 commits from the file history. 2. Print the response to the user. 3. If you decide to print not all commits that were returned to you, notify user about that and ask if he wants to see others.  4. Notify user that you've retrieved all commits from the file history.\"}\n",
      "Here is the Code search result:\n",
      "{'branchName': 'main', 'searchResults': [{'path': 'recommenders/models/newsrec/models/npa.py', 'matches': ['    \"\"\"NPA model(Neural News Recommendation with Attentive Multi-View Learning)\\n\\n    Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang and Xing Xie:\\n    NPA: Neural News Recommendation with Personalized Attention, KDD 2019, ADS track.\\n\\n    Attributes:\\n        word2vec_embedding (numpy.ndarray): Pretrained word embedding matrix.\\n']}, {'path': 'recommenders/models/newsrec/models/nrms.py', 'matches': ['\\n\\nclass NRMSModel(BaseModel):\\n    \"\"\"NRMS model(Neural News Recommendation with Multi-Head Self-Attention)\\n\\n    Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang,and Xing Xie, \"Neural News\\n    Recommendation with Multi-Head Self-Attention\" in Proceedings of the 2019 Conference\\n']}, {'path': 'README.md', 'matches': ['| Multinomial VAE | Collaborative Filtering | Generative model for predicting user/item interactions. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb) |\\n| Neural Recommendation with Long- and Short-term User Representations (LSTUR)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with long- and short-term user interest modeling. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/lstur_MIND.ipynb) |\\n| Neural Recommendation with Attentive Multi-View Learning (NAML)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with attentive multi-view learning. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/naml_MIND.ipynb) |\\n', '| Neural Collaborative Filtering (NCF) | Collaborative Filtering | Deep learning algorithm with enhanced performance for user/item implicit feedback. It works in the CPU/GPU environment.| [Quick start](examples/00_quick_start/ncf_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) |\\n| Neural Recommendation with Personalized Attention (NPA)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with personalized attention network. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/npa_MIND.ipynb) |\\n| Neural Recommendation with Multi-Head Self-Attention (NRMS)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with multi-head self-attention. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/nrms_MIND.ipynb) |\\n']}, {'path': 'examples/00_quick_start/npa_MIND.ipynb', 'matches': ['            \"cell_type\": \"markdown\",\\n            \"metadata\": {},\\n            \"source\": [\\n                \"# NPA: Neural News Recommendation with Personalized Attention\\\\n\",\\n                \"NPA \\\\\\\\[1\\\\\\\\] is a news recommendation model with personalized attention. The core of NPA is a news representation model and a user representation model. In the news representation model we use a CNN network to learn hidden representations of news articles based on their titles. In the user representation model we learn the representations of users based on the representations of their clicked news articles. In addition, a word-level and a news-level personalized attention are used to capture different informativeness for different users.\\\\n\",\\n                \"\\\\n\",\\n                \"## Properties of NPA:\\\\n\",\\n']}, {'path': 'examples/00_quick_start/naml_MIND.ipynb', 'matches': ['            \"source\": [\\n                \"# NAML: Neural News Recommendation with Attentive Multi-View Learning\\\\n\",\\n                \"NAML \\\\\\\\[1\\\\\\\\] is a multi-view news recommendation approach. The core of NAML is a news encoder and a user encoder. The newsencoder is composed of a title encoder, a body encoder, a vert encoder and a subvert encoder. The CNN-based title encoder and body encoder learn title and body representations by capturing words semantic information. After getting news title, body, vert and subvert representations, an attention network is used to aggregate those vectors. In the user encoder, we learn representations of users from their browsed news. Besides, we apply additive attention to learn more informative news and user representations by selecting important words and news.\\\\n\",\\n', '                \"## Properties of NAML:\\\\n\",\\n                \"- NAML is a multi-view neural news recommendation approach.\\\\n\",\\n                \"- It uses news title, news body, news vert and news subvert to get news repersentations. And it uses user historical behaviors to learn user representations.\\\\n\",\\n']}, {'path': 'examples/00_quick_start/nrms_MIND.ipynb', 'matches': ['            \"source\": [\\n                \"# NRMS: Neural News Recommendation with Multi-Head Self-Attention\\\\n\",\\n                \"NRMS \\\\\\\\[1\\\\\\\\] is a neural news recommendation approach with multi-head selfattention. The core of NRMS is a news encoder and a user encoder. In the newsencoder, a multi-head self-attentions is used to learn news representations from news titles by modeling the interactions between words. In the user encoder, we learn representations of users from their browsed news and use multihead self-attention to capture the relatedness between the news. Besides, we apply additive\\\\n\",\\n', '                \"## Reference\\\\n\",\\n                \"\\\\\\\\[1\\\\\\\\] Wu et al. \\\\\"Neural News Recommendation with Multi-Head Self-Attention.\\\\\" in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<br>\\\\n\",\\n                \"\\\\\\\\[2\\\\\\\\] Wu, Fangzhao, et al. \\\\\"MIND: A Large-scale Dataset for News Recommendation\\\\\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html <br>\\\\n\",\\n']}, {'path': 'examples/00_quick_start/lstur_MIND.ipynb', 'matches': ['            \"cell_type\": \"markdown\",\\n            \"metadata\": {},\\n            \"source\": [\\n                \"# LSTUR: Neural News Recommendation with Long- and Short-term User Representations\\\\n\",\\n                \"LSTUR \\\\\\\\[1\\\\\\\\] is a news recommendation approach capturing users\\' both long-term preferences and short-term interests. The core of LSTUR is a news encoder and a user encoder.  In the news encoder, we learn representations of news from their titles. In user encoder, we propose to learn long-term\\\\n\",\\n                \"user representations from the embeddings of their IDs. In addition, we propose to learn short-term user representations from their recently browsed news via GRU network. Besides, we propose two methods to combine\\\\n\",\\n                \"long-term and short-term user representations. The first one is using the long-term user representation to initialize the hidden state of the GRU network in short-term user representation. The second one is concatenating both\\\\n\",\\n']}, {'path': 'examples/00_quick_start/dkn_MIND.ipynb', 'matches': ['            \"source\": [\\n                \"# DKN : Deep Knowledge-Aware Network for News Recommendation\\\\n\",\\n                \"\\\\n\",\\n                \"DKN \\\\\\\\[1\\\\\\\\] is a deep learning model which incorporates information from knowledge graph for better news recommendation. Specifically, DKN uses TransX \\\\\\\\[2\\\\\\\\] method for knowledge graph representation learning, then applies a CNN framework, named KCNN, to combine entity embedding with word embedding and generate a final embedding vector for a news article. CTR prediction is made via an attention-based neural scorer. \\\\n\",\\n                \"\\\\n\",\\n                \"## Properties of DKN:\\\\n\",\\n                \"\\\\n\",\\n']}, {'path': 'recommenders/models/newsrec/models/naml.py', 'matches': ['    \"\"\"NAML model(Neural News Recommendation with Attentive Multi-View Learning)\\n\\n    Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang and Xing Xie,\\n    Neural News Recommendation with Attentive Multi-View Learning, IJCAI 2019\\n\\n    Attributes:\\n        word2vec_embedding (numpy.ndarray): Pretrained word embedding matrix.\\n']}, {'path': 'recommenders/models/newsrec/models/lstur.py', 'matches': ['    \"\"\"LSTUR model(Neural News Recommendation with Multi-Head Self-Attention)\\n\\n    Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu and Xing Xie:\\n    Neural News Recommendation with Long- and Short-term User Representations, ACL 2019\\n\\n    Attributes:\\n        word2vec_embedding (numpy.ndarray): Pretrained word embedding matrix.\\n']}, {'path': 'recommenders/README.md', 'matches': ['* LightGBM\\n* NCF\\n* NewsRec\\n  * Neural Recommendation with Long- and Short-term User Representations (LSTUR)\\n  * Neural Recommendation with Attentive Multi-View Learning (NAML)\\n  * Neural Recommendation with Personalized Attention (NPA)\\n  * Neural Recommendation with Multi-Head Self-Attention (NRMS)\\n']}, {'path': 'examples/02_model_content_based_filtering/dkn_deep_dive.ipynb', 'matches': ['            \"source\": [\\n                \"# DKN : Deep Knowledge-Aware Network for News Recommendation\\\\n\",\\n                \"\\\\n\",\\n                \"DKN \\\\\\\\[1\\\\\\\\] is a deep learning model which incorporates information from knowledge graph for better news recommendation. Specifically, DKN uses TransX \\\\\\\\[2\\\\\\\\] method for knowledge graph representation learning, then applies a CNN framework, named KCNN, to combine entity embedding with word embedding and generate a final embedding vector for a news article. CTR prediction is made via an attention-based neural scorer. \\\\n\",\\n                \"\\\\n\",\\n                \"## Properties of DKN:\\\\n\",\\n                \"\\\\n\",\\n']}, {'path': 'examples/00_quick_start/README.md', 'matches': ['[4] _Restricted Boltzmann Machines for Collaborative Filtering_, Ruslan Salakhutdinov, Andriy Mnih and Geoffrey Hinton. ICML 2007.<br>\\n[5] _Wide & Deep Learning for Recommender Systems_, Heng-Tze Cheng et al., arXiv:1606.07792 2016. <br>\\n[6] _A unified framework for structured low-rank matrix learning_, Pratik Jawanpuria and Bamdev Mishra, In International Conference on Machine Learning, 2018. <br>\\n[7] _NAML: Neural News Recommendation with Attentive Multi-View Learning_, Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang and Xing Xie. IJCAI 2019.<br>\\n[8] _NRMS: Neural News Recommendation with Multi-Head Self-Attention_, Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, Xing Xie. in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).<br>\\n[9] _LSTUR: Neural News Recommendation with Long- and Short-term User Representations_, Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu and Xing Xie. ACL 2019.<br>\\n[10] _NPA: Neural News Recommendation with Personalized Attention_, Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang and Xing Xie. KDD 2019, ADS track.<br>\\n']}, {'path': 'examples/07_tutorials/KDD2020-tutorial/step3_run_dkn.ipynb', 'matches': ['            \"metadata\": {},\\n            \"source\": [\\n                \"# DKN : Deep Knowledge-Aware Network for News Recommendation\\\\n\",\\n                \"DKN \\\\\\\\[1\\\\\\\\] is a deep learning model which incorporates information from knowledge graph for better news recommendation. Specifically, DKN uses TransX \\\\\\\\[2\\\\\\\\] method for knowledge graph representaion learning, then applies a CNN framework, named KCNN, to combine entity embedding with word embedding and generate a final embedding vector for a news article. CTR prediction is made via an attention-based neural scorer. \\\\n\",\\n                \"<img src=\\\\\"https://recodatasets.z20.web.core.windows.net/kdd2020/images%2FDKN-introduction-pic.JPG\\\\\" width=\\\\\"600\\\\\">\\\\n\",\\n                \"\\\\n\",\\n                \"## Properties of DKN:\\\\n\",\\n']}, {'path': 'examples/07_tutorials/KDD2020-tutorial/step2_pretraining-embeddings.ipynb', 'matches': ['                \"## Reference\\\\n\",\\n                \"\\\\\\\\[1\\\\\\\\] Wang, Hongwei, et al. \\\\\"DKN: Deep Knowledge-Aware Network for News Recommendation.\\\\\" Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2018.<br>\\\\n\",\\n                \"\\\\\\\\[2\\\\\\\\] Knowledge Graph Embeddings including TransE, TransH, TransR and PTransE. https://github.com/thunlp/KB2E <br>\\\\n\",\\n', '                \"\\\\\\\\[3\\\\\\\\] GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/projects/glove/ <br>\\\\n\",\\n                \"\\\\\\\\[4\\\\\\\\] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (NIPS’13). Curran Associates Inc., Red Hook, NY, USA, 3111–3119. <br>\\\\n\",\\n                \"\\\\\\\\[5\\\\\\\\] Gensim  Word2vec embeddings : https://radimrehurek.com/gensim/models/word2vec.html <br>\"\\n']}], 'assistantNextSteps': '1. Analyze the response and returned matches.2. If they answer users question, print the response. If not, try to either search with different keywords, or try to get the full file contents.'}\n",
      "Here is the Commits search result:\n",
      "{'commits': [{'url': 'https://github.com/recommenders-team/recommenders/commit/d090846697b7837914e10f353b5bc80dff3f5a0f', 'message': 'Documentation working\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['Documentation working\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/69a485c50f950adcf4da7e8189725224f5850a57', 'message': 'Merge pull request #2051 from recommenders-team/miguel/jupyter_book\\n\\nNew documentation with Jupyter book', 'matches': ['Merge pull request #2051 from recommenders-team/miguel/jupyter_book\\n\\nNew documentation with Jupyter book']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/310370b2a0fd3366bce264b9b007ad6d6af330e0', 'message': 'Merge pull request #2057 from recommenders-team/staging\\n\\nStaging to main: New documentation with Jupyter Book', 'matches': ['Merge pull request #2057 from recommenders-team/staging\\n\\nStaging to main: New documentation with Jupyter Book']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/32ee9d8a942a282fa9d3db95d8cee69d32a24556', 'message': 'Change the way we compile the documentation\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['Change the way we compile the documentation\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/769900801872e9f381aebeaff7ab6536f5c7614c', 'message': 'Update documentation badge\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['Update documentation badge\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/9263c7d3914b3790eab182d5ac91a9181c84f825', 'message': 'actions to automatically update documentation\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['actions to automatically update documentation\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/006a33d8c78b4e806d590fd1d366d60e73d194e2', 'message': 'actions to automatically update documentation\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['actions to automatically update documentation\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/9474dff874773430384606710fe91a27c7ce4ff1', 'message': 'Creating documentation\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['Creating documentation\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/b3217f813a0e0eb8d887ac38ba919757ae7966a4', 'message': 'actions to automatically update documentation :bug:\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['actions to automatically update documentation :bug:\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/5a221df288a6bf1d2a5bbe7655f3abd967f9365a', 'message': 'actions to automatically update documentation :bug:\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['actions to automatically update documentation :bug:\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/b887e439f743a5a02f7719cf7888434ea884a3e8', 'message': 'Automatic build of documentation\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['Automatic build of documentation\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/1d308a4e0a538b3fa2dc0dfb94e9c46c2f385266', 'message': 'Merge pull request #2055 from recommenders-team/staging\\n\\nStaging to main: Documentation update, algorithm classification update and fix error in MIND', 'matches': ['Merge pull request #2055 from recommenders-team/staging\\n\\nStaging to main: Documentation update, algorithm classification update and fix error in MIND']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/42cd1525b7a59fca9bf39b587354b43b38f6a4da', 'message': 'Automatic build of documentation deps\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['Automatic build of documentation deps\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/23fa87c4d275e949f93cbc8cdccc60b9e8512154', 'message': 'Automatic build of documentation deps\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['Automatic build of documentation deps\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/20720f59c99d9967f83162368fa988d7e6fcc2a0', 'message': 'Automatic build of documentation deps\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['Automatic build of documentation deps\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/87225505814cbaba0de3e4d0b825268227817aa4', 'message': 'Automatic build of documentation dev\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>', 'matches': ['Automatic build of documentation dev\\n\\nSigned-off-by: miguelgfierro <miguelgfierro@users.noreply.github.com>']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/787ae309ec78a9b2b1f58931931cb117affc4ea9', 'message': 'Merge pull request #1940 from microsoft/staging\\n\\nStaging to main: fix tests and update documentation', 'matches': ['Merge pull request #1940 from microsoft/staging\\n\\nStaging to main: fix tests and update documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/8ee1ed3ac0db04321b064edb6f10d6af0bb318fd', 'message': 'Merge pull request #1908 from microsoft/staging\\n\\nStaging to main: improvements in testing pipeline and documentation', 'matches': ['Merge pull request #1908 from microsoft/staging\\n\\nStaging to main: improvements in testing pipeline and documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/53c01881843b256ab33bffffa91bdf6e363e6f6a', 'message': 'Merge pull request #1729 from microsoft/pradjoshi/azureml_doc\\n\\nUpdate AzureML test documentation', 'matches': ['Merge pull request #1729 from microsoft/pradjoshi/azureml_doc\\n\\nUpdate AzureML test documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/dbf7ed08d36b4c77e5455c2c3ba165d6308aeaaf', 'message': 'update loss function documentation', 'matches': ['update loss function documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/772185788ec037a9e53a2e498586bcaecb224798', 'message': 'Merge pull request #1690 from microsoft/bug/doc\\n\\nFixed readthedocs bug and added SASRec and SSEPT documentation', 'matches': ['Merge pull request #1690 from microsoft/bug/doc\\n\\nFixed readthedocs bug and added SASRec and SSEPT documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/35178cef961d617949bd1878c037c246d37b3bfc', 'message': 'Change Java version in documentation', 'matches': ['Change Java version in documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/1e652115bc8e76b3c5ec1c7c33e5bd935376ff8f', 'message': 'Update dev documentation', 'matches': ['Update dev documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/6e09f4e4275158873d47f567bc9dce030f697479', 'message': 'danb27/sar_single_node_improvements: allow for getting most frequent users, similar users, and fix self.item_frequencies to actually use frequencies as stated in documentation', 'matches': ['danb27/sar_single_node_improvements: allow for getting most frequent users, similar users, and fix self.item_frequencies to actually use frequencies as stated in documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/07b9399bd7c90e2e27f953dbf7960d11578a3b0d', 'message': 'Update test-running documentation', 'matches': ['Update test-running documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/99379a65b34b1e2e42ce293244eb9914fe71f672', 'message': 'Merge pull request #859 from microsoft/documentation\\n\\nDocumentation of Recommenders', 'matches': ['Merge pull request #859 from microsoft/documentation\\n\\nDocumentation of Recommenders']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/6594a96c58936f7ac76dc324dcd9a79ac59e99d6', 'message': 'Merge pull request #859 from microsoft/documentation\\n\\nDocumentation of Recommenders', 'matches': ['Merge pull request #859 from microsoft/documentation\\n\\nDocumentation of Recommenders']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/dc7de0d875491e7d89c4b106a231c543b635606d', 'message': 'rbm documentation', 'matches': ['rbm documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/107e44fed01b2cebabb49d37ab37cbc4f09d577f', 'message': 'rbm documentation', 'matches': ['rbm documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/7e707419b8a340ca96c8c06d756d7966d7e0e25b', 'message': 'review documentation', 'matches': ['review documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/f649868e087eef86dc63c1d909d42ca7198b59b1', 'message': 'review documentation', 'matches': ['review documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/3fe9507832a6b293e9c0c92dc0f1e9a8640ed5a3', 'message': 'fixed documentation', 'matches': ['fixed documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/94e81cd357a18b7cc1e0d180284b7c047e5228ef', 'message': 'fixed documentation', 'matches': ['fixed documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/b93d8fa391a4ffaa04f8a88ef99595a7ac73c257', 'message': 'fixed documentation', 'matches': ['fixed documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/bd14234cb7eb066ee4ea0ecedc08e13ab99cbc06', 'message': 'fixed documentation', 'matches': ['fixed documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/015ea427a04c495db6ec2f5e99a71024749bc0c1', 'message': 'review documentation :boom:', 'matches': ['review documentation :boom:']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/31760f9e07ceb611519f087cfbaf86a7ff2250c0', 'message': 'review documentation :boom:', 'matches': ['review documentation :boom:']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/95aa96228adb41195f56f9f658777e33126a5ace', 'message': 'change in the documentation', 'matches': ['change in the documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/1af5e92508f5dc7b6aa1d8294f5a0937f4c28cbd', 'message': 'change in the documentation', 'matches': ['change in the documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/5174df94ff097c3d6e4bf755731486f7f2c788d0', 'message': 'Merge pull request #80 from Microsoft/nikhil/documentation\\n\\nnotebook documentation in single node SAR and educational SAR notebooks\\r\\n\\r\\nfixes #57', 'matches': ['Merge pull request #80 from Microsoft/nikhil/documentation\\n\\nnotebook documentation in single node SAR and educational SAR notebooks\\r\\n\\r\\nfixes #57']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/23b4dc156fbb85add0849a4ec8c2793ef7fd89d7', 'message': 'Merge pull request #80 from Microsoft/nikhil/documentation\\n\\nnotebook documentation in single node SAR and educational SAR notebooks\\r\\n\\r\\nfixes #57', 'matches': ['Merge pull request #80 from Microsoft/nikhil/documentation\\n\\nnotebook documentation in single node SAR and educational SAR notebooks\\r\\n\\r\\nfixes #57']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/2ba26b348db7f60e720882cebe0ccc290943589e', 'message': 'fixed documentation for RBM', 'matches': ['fixed documentation for RBM']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/cb2eb8d71b76ec130ae6cf48a9b1b9c000db38e0', 'message': 'fixed documentation for RBM', 'matches': ['fixed documentation for RBM']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/0f55c7f3ef6bfd01a3629e7f99f3dde2b25c0d23', 'message': 'updated documentation', 'matches': ['updated documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/248f91bb5ececb3e09e93315c7799ac97405b26f', 'message': 'updated documentation', 'matches': ['updated documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/aee5f37ae56b44da61e3b7e4b68444bb87a53f5b', 'message': 'Add sphinx documentation for wikidata', 'matches': ['Add sphinx documentation for wikidata']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/65855669e21116218bf9ea96c1e43a370954ba71', 'message': 'Add sphinx documentation for wikidata', 'matches': ['Add sphinx documentation for wikidata']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/1a7ae02addc783b7707aebe7870ef0d07324bba2', 'message': \"Merge branch 'staging' into documentation\", 'matches': [\"Merge branch 'staging' into documentation\"]}, {'url': 'https://github.com/recommenders-team/recommenders/commit/aa660197d14d25cea9f4f0b3145c3b602af7a6b9', 'message': \"Merge branch 'staging' into documentation\", 'matches': [\"Merge branch 'staging' into documentation\"]}, {'url': 'https://github.com/recommenders-team/recommenders/commit/510c88b3167b3fb5ed6415f4d472fdf87cfd201f', 'message': \"Merge branch 'staging' into documentation\", 'matches': [\"Merge branch 'staging' into documentation\"]}, {'url': 'https://github.com/recommenders-team/recommenders/commit/61664516ae108a71d8e083c80b89eb35590587aa', 'message': \"Merge branch 'staging' into documentation\", 'matches': [\"Merge branch 'staging' into documentation\"]}, {'url': 'https://github.com/recommenders-team/recommenders/commit/848168cb4953692d835de47d73940a70a034a759', 'message': 'sar documentation', 'matches': ['sar documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/eff7aba831cb8bdcd2b4a5dbed8f6c4def67f076', 'message': 'sar documentation', 'matches': ['sar documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/f57be9d20180cfd6fcf6b2ede596b2adeefca4e2', 'message': 'fixing documentation :bug: related to #817', 'matches': ['fixing documentation :bug: related to #817']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/8660a1949990c9a8ea599fb96ebd7877669efc00', 'message': 'fixing documentation :bug: related to #817', 'matches': ['fixing documentation :bug: related to #817']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/81acd1cd9a788268ccbf82e86819ba5a5ecc24eb', 'message': 'added some documentation', 'matches': ['added some documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/7c0472dae54efc3cea16a2674cdb4dff2d00b43b', 'message': 'added some documentation', 'matches': ['added some documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/6a91ccbc829c8bbd07d0fabe492f791f32b50fad', 'message': 'improved documentation; reverted unit test refactoring', 'matches': ['improved documentation; reverted unit test refactoring']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/c6a23a0a26bb1a7c29b516c38d8795857d84e8c7', 'message': 'improved documentation; reverted unit test refactoring', 'matches': ['improved documentation; reverted unit test refactoring']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/7333b1852b7f4e24a5a1d5622a2b026143f3414f', 'message': \"Merge branch 'staging' into nikhil/documentation\", 'matches': [\"Merge branch 'staging' into nikhil/documentation\"]}, {'url': 'https://github.com/recommenders-team/recommenders/commit/2e904303b10c9bb9fc55a67da4ffb46b0448c710', 'message': \"Merge branch 'staging' into nikhil/documentation\", 'matches': [\"Merge branch 'staging' into nikhil/documentation\"]}, {'url': 'https://github.com/recommenders-team/recommenders/commit/cedd2dff450481617214e547f535fab50908329a', 'message': \"Merge branch 'staging' into nikhil/documentation\", 'matches': [\"Merge branch 'staging' into nikhil/documentation\"]}, {'url': 'https://github.com/recommenders-team/recommenders/commit/ee29c9e0a7d2602c8f55851fb11de597b89d503b', 'message': \"Merge branch 'staging' into nikhil/documentation\", 'matches': [\"Merge branch 'staging' into nikhil/documentation\"]}, {'url': 'https://github.com/recommenders-team/recommenders/commit/fe8c389eab55be1c27731234dc0e045043583ea7', 'message': 'documentation, improved tests in wide and deep model and fix :bug: in tests', 'matches': ['documentation, improved tests in wide and deep model and fix :bug: in tests']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/287718bc97cd9b25b414352371917848f50345fc', 'message': 'documentation, improved tests in wide and deep model and fix :bug: in tests', 'matches': ['documentation, improved tests in wide and deep model and fix :bug: in tests']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/56b6451bf2ebf4dec9937ba68202dabe999887f9', 'message': 'Merge pull request #879 from microsoft/miguelgfierro-patch-1\\n\\nAdd sphinx documentation for wikidata', 'matches': ['Merge pull request #879 from microsoft/miguelgfierro-patch-1\\n\\nAdd sphinx documentation for wikidata']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/7339405996eaf04e1143c77b04c94e5f79e6a979', 'message': 'Merge pull request #879 from microsoft/miguelgfierro-patch-1\\n\\nAdd sphinx documentation for wikidata', 'matches': ['Merge pull request #879 from microsoft/miguelgfierro-patch-1\\n\\nAdd sphinx documentation for wikidata']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/1906408a4fb57f1349e63e75ae38fc87c6574e9f', 'message': 'Merge pull request #482 from Microsoft/doc/tests\\n\\nadded papermill documentation', 'matches': ['Merge pull request #482 from Microsoft/doc/tests\\n\\nadded papermill documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/b0a177e6feff4097c7b4aa7cf85b1adc904acb16', 'message': 'Merge pull request #482 from Microsoft/doc/tests\\n\\nadded papermill documentation', 'matches': ['Merge pull request #482 from Microsoft/doc/tests\\n\\nadded papermill documentation']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/71a38d422c00329f8f8226ea24ad6260b1b7b4e9', 'message': 'Merge pull request #867 from microsoft/staging\\n\\nStaging to master after documentation and other changes', 'matches': ['Merge pull request #867 from microsoft/staging\\n\\nStaging to master after documentation and other changes']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/67ebc0d740453b22c7e87e331a4ee57a790909f6', 'message': 'adding skips for mmlspark_lightgbm tests on windows, updating test documentation to track skips', 'matches': ['adding skips for mmlspark_lightgbm tests on windows, updating test documentation to track skips']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/75695d5d7ff88e7836e68e37bacd191117ba13d2', 'message': 'adding skips for mmlspark_lightgbm tests on windows, updating test documentation to track skips', 'matches': ['adding skips for mmlspark_lightgbm tests on windows, updating test documentation to track skips']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/4fef4f4594b66e30bbe8a784a45391aa4ea31218', 'message': 'fixed capitalization of pyspark\\n\\nPySpark documentation has it capitalized as \"PySpark\" rather than \"pySpark\".', 'matches': ['fixed capitalization of pyspark\\n\\nPySpark documentation has it capitalized as \"PySpark\" rather than \"pySpark\".']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/46520520ece0eadc1c3647998e8dfce93774fa0a', 'message': 'fixed capitalization of pyspark\\n\\nPySpark documentation has it capitalized as \"PySpark\" rather than \"pySpark\".', 'matches': ['fixed capitalization of pyspark\\n\\nPySpark documentation has it capitalized as \"PySpark\" rather than \"pySpark\".']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/65d2b600acb75832c0194282278b87642a63671f', 'message': 'Merge pull request #325 from Microsoft/sgraham/readme\\n\\nChanges to readme documentation throughout repo', 'matches': ['Merge pull request #325 from Microsoft/sgraham/readme\\n\\nChanges to readme documentation throughout repo']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/df94ca603445db0780959d7865aa2e3e1a456d28', 'message': 'Merge pull request #325 from Microsoft/sgraham/readme\\n\\nChanges to readme documentation throughout repo', 'matches': ['Merge pull request #325 from Microsoft/sgraham/readme\\n\\nChanges to readme documentation throughout repo']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/7d5070ef5a271b11ea8776a0c9472a23c3e98af8', 'message': 'Adding default environment yaml.\\n\\nThe jupyter tool \"jupyter-repo2docker\" expects an environment.yml file in the root of the repo directory. While there may be other configs that could be added, adding this file would also enable creating a base conda env with the default commands, without changing any of the existing documentation.', 'matches': ['Adding default environment yaml.\\n\\nThe jupyter tool \"jupyter-repo2docker\" expects an environment.yml file in the root of the repo directory. While there may be other configs that could be added, adding this file would also enable creating a base conda env with the default commands, without changing any of the existing documentation.']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/f69f0140614bc0bb2a4506fb59dbc6f19449335a', 'message': 'Docker Support (#718)\\n\\n* DOCKER: add pyspark docker file\\r\\n\\r\\n* DOCKER: remove unused line\\r\\n\\r\\n* DOCKER: remove old file\\r\\n\\r\\n* DOCKER: add SETUP text\\r\\n\\r\\n* DOCKER: add azureml`\\r\\n\\r\\n* DOCKER: udpate dockerfile\\r\\n\\r\\n* DOCKER: use a branch of the repo\\r\\n\\r\\n* SETUP: update setup\\r\\n\\r\\n* DOCKER: update dockerfile\\r\\n\\r\\n* DOC: update setup\\r\\n\\r\\n* DOCKER: one that binds all\\r\\n\\r\\n* SETUP: update docker use\\r\\n\\r\\n* DOCKER: move to top level\\r\\n\\r\\n* SETUP: use a different base name\\r\\n\\r\\n* DOCKER: use the same keywords in the repo for environment arg\\r\\n\\r\\n* SETUP: update environment variable names\\r\\n\\r\\n* updating dockerfile to use multistage build and adding readme\\r\\n\\r\\n* adding full stage\\r\\n\\r\\n* fixing documentation\\r\\n\\r\\n* adding info for running full env\\r\\n\\r\\n* README: update notes for exporting environment on certain platform\\r\\n\\r\\n* README: updated with example on Windows\\r\\n\\r\\n* README: fix typo', 'matches': ['Docker Support (#718)\\n\\n* DOCKER: add pyspark docker file\\r\\n\\r\\n* DOCKER: remove unused line\\r\\n\\r\\n* DOCKER: remove old file\\r\\n\\r\\n* DOCKER: add SETUP text\\r\\n\\r\\n* DOCKER: add azureml`\\r\\n\\r\\n* DOCKER: udpate dockerfile\\r\\n\\r\\n* DOCKER: use a branch of the repo\\r\\n\\r\\n* SETUP: update setup\\r\\n\\r\\n* DOCKER: update dockerfile\\r\\n\\r\\n* DOC: update setup\\r\\n\\r\\n* DOCKER: one that binds all\\r\\n\\r\\n* SETUP: update docker use\\r\\n\\r\\n* DOCKER: move to top level\\r\\n\\r\\n* SETUP: use a different base name\\r\\n\\r\\n* DOCKER: use the same keywords in the repo for environment arg\\r\\n\\r\\n* SETUP: update environment variable names\\r\\n\\r\\n* updating dockerfile to use multistage build and adding readme\\r\\n\\r\\n* adding full stage\\r\\n\\r\\n* fixing documentation\\r\\n\\r\\n* adding info for running full env\\r\\n\\r\\n* README: update notes for exporting environment on certain platform\\r\\n\\r\\n* README: updated with example on Windows\\r\\n\\r\\n* README: fix typo']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/83e0d69093d8f40b4b55dd13d87a9b12141deac4', 'message': 'Docker Support (#718)\\n\\n* DOCKER: add pyspark docker file\\r\\n\\r\\n* DOCKER: remove unused line\\r\\n\\r\\n* DOCKER: remove old file\\r\\n\\r\\n* DOCKER: add SETUP text\\r\\n\\r\\n* DOCKER: add azureml`\\r\\n\\r\\n* DOCKER: udpate dockerfile\\r\\n\\r\\n* DOCKER: use a branch of the repo\\r\\n\\r\\n* SETUP: update setup\\r\\n\\r\\n* DOCKER: update dockerfile\\r\\n\\r\\n* DOC: update setup\\r\\n\\r\\n* DOCKER: one that binds all\\r\\n\\r\\n* SETUP: update docker use\\r\\n\\r\\n* DOCKER: move to top level\\r\\n\\r\\n* SETUP: use a different base name\\r\\n\\r\\n* DOCKER: use the same keywords in the repo for environment arg\\r\\n\\r\\n* SETUP: update environment variable names\\r\\n\\r\\n* updating dockerfile to use multistage build and adding readme\\r\\n\\r\\n* adding full stage\\r\\n\\r\\n* fixing documentation\\r\\n\\r\\n* adding info for running full env\\r\\n\\r\\n* README: update notes for exporting environment on certain platform\\r\\n\\r\\n* README: updated with example on Windows\\r\\n\\r\\n* README: fix typo', 'matches': ['Docker Support (#718)\\n\\n* DOCKER: add pyspark docker file\\r\\n\\r\\n* DOCKER: remove unused line\\r\\n\\r\\n* DOCKER: remove old file\\r\\n\\r\\n* DOCKER: add SETUP text\\r\\n\\r\\n* DOCKER: add azureml`\\r\\n\\r\\n* DOCKER: udpate dockerfile\\r\\n\\r\\n* DOCKER: use a branch of the repo\\r\\n\\r\\n* SETUP: update setup\\r\\n\\r\\n* DOCKER: update dockerfile\\r\\n\\r\\n* DOC: update setup\\r\\n\\r\\n* DOCKER: one that binds all\\r\\n\\r\\n* SETUP: update docker use\\r\\n\\r\\n* DOCKER: move to top level\\r\\n\\r\\n* SETUP: use a different base name\\r\\n\\r\\n* DOCKER: use the same keywords in the repo for environment arg\\r\\n\\r\\n* SETUP: update environment variable names\\r\\n\\r\\n* updating dockerfile to use multistage build and adding readme\\r\\n\\r\\n* adding full stage\\r\\n\\r\\n* fixing documentation\\r\\n\\r\\n* adding info for running full env\\r\\n\\r\\n* README: update notes for exporting environment on certain platform\\r\\n\\r\\n* README: updated with example on Windows\\r\\n\\r\\n* README: fix typo']}, {'url': 'https://github.com/recommenders-team/recommenders/commit/e8ee8aa63b80dc972fd901e841d84096f730f3ba', 'message': 'Staging to master (#882)\\n\\n* new file with wikidata functions\\r\\n\\r\\n* fix in json extraction\\r\\n\\r\\n* new notebook with wikidata use examples\\r\\n\\r\\n* retry request with lowercase in case of failure\\r\\n\\r\\n* WIP: example creating KG from movielens entities\\r\\n\\r\\n* introduced new step to retrieve first page title from a text query in wikipedia\\r\\n\\r\\n* updated movielens links extraction using wikidata\\r\\n\\r\\n* adapted docstrings for sphinx and removed parenthesis from output\\r\\n\\r\\n* added description and labels to nodes to graph preview\\r\\n\\r\\n* https://github.com/microsoft/recommenders/pull/778#discussion_r294565020 new format for queries\\r\\n\\r\\n* raising exceptions in requests and using get() to retrieve dict values\\r\\n\\r\\n* moved imports to first cell and movielens size as a parameter\\r\\n\\r\\n* output file name as paramenter\\r\\n\\r\\n* DATA: update sum check\\r\\n\\r\\n* adding unit test for sum to 1 issue\\r\\n\\r\\n* improved description and adapted to tests\\r\\n\\r\\n* improved Exception descriptions\\r\\n\\r\\n* integration tests\\r\\n\\r\\n* unit tests\\r\\n\\r\\n* added wikidata_KG to conftest\\r\\n\\r\\n* changed name notebook\\r\\n\\r\\n* *NOTE: Adding  shows the computation time of all tests.*\\r\\n\\r\\n* imports up\\r\\n\\r\\n* Update wikidata.py\\r\\n\\r\\n* changed default parameter of sample for tests\\r\\n\\r\\n* Add sphinx documentation for wikidata\\r\\n\\r\\n* modified parameter extraction for tests\\r\\n\\r\\n* added parameters tag to cell\\r\\n\\r\\n* changed default sampling to test parameters in test\\r\\n\\r\\n* notebook cleaned cells output\\r\\n\\r\\n* Docker Support (#718)\\r\\n\\r\\n* DOCKER: add pyspark docker file\\r\\n\\r\\n* DOCKER: remove unused line\\r\\n\\r\\n* DOCKER: remove old file\\r\\n\\r\\n* DOCKER: add SETUP text\\r\\n\\r\\n* DOCKER: add azureml`\\r\\n\\r\\n* DOCKER: udpate dockerfile\\r\\n\\r\\n* DOCKER: use a branch of the repo\\r\\n\\r\\n* SETUP: update setup\\r\\n\\r\\n* DOCKER: update dockerfile\\r\\n\\r\\n* DOC: update setup\\r\\n\\r\\n* DOCKER: one that binds all\\r\\n\\r\\n* SETUP: update docker use\\r\\n\\r\\n* DOCKER: move to top level\\r\\n\\r\\n* SETUP: use a different base name\\r\\n\\r\\n* DOCKER: use the same keywords in the repo for environment arg\\r\\n\\r\\n* SETUP: update environment variable names\\r\\n\\r\\n* updating dockerfile to use multistage build and adding readme\\r\\n\\r\\n* adding full stage\\r\\n\\r\\n* fixing documentation\\r\\n\\r\\n* adding info for running full env\\r\\n\\r\\n* README: update notes for exporting environment on certain platform\\r\\n\\r\\n* README: updated with example on Windows\\r\\n\\r\\n* README: fix typo', 'matches': ['Staging to master (#882)\\n\\n* new file with wikidata functions\\r\\n\\r\\n* fix in json extraction\\r\\n\\r\\n* new notebook with wikidata use examples\\r\\n\\r\\n* retry request with lowercase in case of failure\\r\\n\\r\\n* WIP: example creating KG from movielens entities\\r\\n\\r\\n* introduced new step to retrieve first page title from a text query in wikipedia\\r\\n\\r\\n* updated movielens links extraction using wikidata\\r\\n\\r\\n* adapted docstrings for sphinx and removed parenthesis from output\\r\\n\\r\\n* added description and labels to nodes to graph preview\\r\\n\\r\\n* https://github.com/microsoft/recommenders/pull/778#discussion_r294565020 new format for queries\\r\\n\\r\\n* raising exceptions in requests and using get() to retrieve dict values\\r\\n\\r\\n* moved imports to first cell and movielens size as a parameter\\r\\n\\r\\n* output file name as paramenter\\r\\n\\r\\n* DATA: update sum check\\r\\n\\r\\n* adding unit test for sum to 1 issue\\r\\n\\r\\n* improved description and adapted to tests\\r\\n\\r\\n* improved Exception descriptions\\r\\n\\r\\n* integration tests\\r\\n\\r\\n* unit tests\\r\\n\\r\\n* added wikidata_KG to conftest\\r\\n\\r\\n* changed name notebook\\r\\n\\r\\n* *NOTE: Adding  shows the computation time of all tests.*\\r\\n\\r\\n* imports up\\r\\n\\r\\n* Update wikidata.py\\r\\n\\r\\n* changed default parameter of sample for tests\\r\\n\\r\\n* Add sphinx documentation for wikidata\\r\\n\\r\\n* modified parameter extraction for tests\\r\\n\\r\\n* added parameters tag to cell\\r\\n\\r\\n* changed default sampling to test parameters in test\\r\\n\\r\\n* notebook cleaned cells output\\r\\n\\r\\n* Docker Support (#718)\\r\\n\\r\\n* DOCKER: add pyspark docker file\\r\\n\\r\\n* DOCKER: remove unused line\\r\\n\\r\\n* DOCKER: remove old file\\r\\n\\r\\n* DOCKER: add SETUP text\\r\\n\\r\\n* DOCKER: add azureml`\\r\\n\\r\\n* DOCKER: udpate dockerfile\\r\\n\\r\\n* DOCKER: use a branch of the repo\\r\\n\\r\\n* SETUP: update setup\\r\\n\\r\\n* DOCKER: update dockerfile\\r\\n\\r\\n* DOC: update setup\\r\\n\\r\\n* DOCKER: one that binds all\\r\\n\\r\\n* SETUP: update docker use\\r\\n\\r\\n* DOCKER: move to top level\\r\\n\\r\\n* SETUP: use a different base name\\r\\n\\r\\n* DOCKER: use the same keywords in the repo for environment arg\\r\\n\\r\\n* SETUP: update environment variable names\\r\\n\\r\\n* updating dockerfile to use multistage build and adding readme\\r\\n\\r\\n* adding full stage\\r\\n\\r\\n* fixing documentation\\r\\n\\r\\n* adding info for running full env\\r\\n\\r\\n* README: update notes for exporting environment on certain platform\\r\\n\\r\\n* README: updated with example on Windows\\r\\n\\r\\n* README: fix typo']}], 'totalCount': 80, 'assistantNextSteps': \"1. Print that you've successfully retrieved 80 most relevant commits. 2. Print the response to the user. Suggest user to refine his search if he is interested in other commits.\"}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# find repos\u001b[39;00m\n\u001b[1;32m     37\u001b[0m search_keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecommenders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 38\u001b[0m test_find_repos_url \u001b[38;5;241m=\u001b[39m \u001b[43mfind_repos\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearchKeywords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_keywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere are the repositories found:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_find_repos_url)\n",
      "Cell \u001b[0;32mIn[68], line 174\u001b[0m, in \u001b[0;36mfind_repos\u001b[0;34m(searchKeywords, language)\u001b[0m\n\u001b[1;32m    169\u001b[0m find_repos_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/search/repository\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearchKeywords\u001b[39m\u001b[38;5;124m'\u001b[39m: searchKeywords,\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m: language\n\u001b[1;32m    173\u001b[0m }\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_api_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_repos_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgithub_token\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 32\u001b[0m, in \u001b[0;36mmake_api_request\u001b[0;34m(endpoint_url, params, github_token)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mMakes a POST request to the specified API endpoint with given parameters and returns the processed JSON response.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mRemoves the 'usefulUrls' field from the response if it exists. In case of an error, returns an error message.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m:return: A dictionary representing the JSON response from the API or a string containing an error message.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgithub_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m }\n\u001b[0;32m---> 32\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     35\u001b[0m     response_json \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/IE/DevOps_Assignement1/Github-Reccomendation-System/.venv/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IE/DevOps_Assignement1/Github-Reccomendation-System/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IE/DevOps_Assignement1/Github-Reccomendation-System/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/IE/DevOps_Assignement1/Github-Reccomendation-System/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/IE/DevOps_Assignement1/Github-Reccomendation-System/.venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/IE/DevOps_Assignement1/Github-Reccomendation-System/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/IE/DevOps_Assignement1/Github-Reccomendation-System/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/IE/DevOps_Assignement1/Github-Reccomendation-System/.venv/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Test the API functions\n",
    "\"\"\"\n",
    "# Structure retrieval\n",
    "test_repo_structure_url = get_repo_structure(url=\"https://github.com/recommenders-team/recommenders\")\n",
    "print(\"Here is the structure of the repository:\")\n",
    "print(test_repo_structure_url)\n",
    "\n",
    "# Content retrieval\n",
    "test_repo_content_url = get_repo_content(url=\"https://github.com/recommenders-team/recommenders\", filePaths=[\".github/.codecov.yml\"])\n",
    "print(\"Here is the content of the repository:\")\n",
    "print(test_repo_content_url)\n",
    "\n",
    "# Branches retrieval\n",
    "test_repo_branches_url = get_repo_branches(url=\"https://github.com/RecandChat/CodeCompass\")\n",
    "print(\"Here are the branches of the repository:\")\n",
    "print(test_repo_branches_url)\n",
    "\n",
    "# Commit history retrieval\n",
    "test_commit_history_url = get_commit_history(url=\"https://github.com/recommenders-team/recommenders\", filePath=\".devcontainer/devcontainer.json\")\n",
    "print(\"Here is the commit history of the repository:\")\n",
    "print(test_commit_history_url)\n",
    "\n",
    "# Code search\n",
    "search_keywords = [\"Neural News Recommendation\"]\n",
    "test_search_repo_code_url = search_repo_code(url=\"https://github.com/recommenders-team/recommenders\", searchKeywords=search_keywords)\n",
    "print(\"Here is the Code search result:\")\n",
    "print(test_search_repo_code_url)\n",
    "\n",
    "# Commits search\n",
    "search_keywords = [\"Documentation\"]\n",
    "test_search_repo_commits_url = search_repo_commits(url=\"https://github.com/recommenders-team/recommenders\", searchKeywords=search_keywords)\n",
    "print(\"Here is the Commits search result:\")\n",
    "print(test_search_repo_commits_url)\n",
    "\n",
    "# find repos\n",
    "search_keywords = [\"recommenders\"]\n",
    "test_find_repos_url = find_repos(searchKeywords=search_keywords)\n",
    "print(\"Here are the repositories found:\")\n",
    "print(test_find_repos_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "### General Instructions when using the plugin\n",
    "\n",
    "- Always pass the whole URL provided by the user to the action. URL may include query parameters or references as well. ALWAYS pass them.\n",
    "- Never execute multiple functions sequentially without first informing the user about the completed action and the next intended action.\n",
    "- Carefully ascertain the user's request to determine which flow to implement\n",
    "- When generating a response, provide links to files in the Github repository instead of just file names\n",
    "- Render useful links at the footer of the response as a links.  All links should be rendered on the same line. Render them only when you've finished with your response, ignore rendering useful links if you plan need to make more requests to the plugin.\n",
    "\n",
    "### End of General Instructions when using the plugin\n",
    "\n",
    "### Supported Flows\n",
    "\n",
    "The AskTheCode plugin is designed to facilitate interaction with Github repositories through four distinct flows. Each flow serves a specific use case and must be employed accordingly to ensure accurate and efficient results.\n",
    "\n",
    "1. Repository Structure Query Flow\n",
    "\n",
    "When a user requests information about the general structure or specific details within a repository, initiate this flow. It involves:\n",
    "- Querying the repository to obtain its structure. This may require multiple queries for larger repositories. After each query, summarize the outcome and notify the user before proceeding to the next request.\n",
    "- When the response contains the nextStep field and it equals to \"GetRepositoryStructure\" - this means that you are not yet ready to query the file contents and you rather need to request the structure of a more relevant subdirectories.\n",
    "- Once the structure is ascertained, proceed to query for the contents of the files that are likely to contain the information relevant to the user's question.\n",
    "\n",
    "2. Search Flow\n",
    "\n",
    "Utilize this to assist users in locating specific elements within GitHub repositories. This flow includes searches for code, commits, issues, and entire repositories. Follow these instructions based on the user's request:\n",
    "\n",
    "2.1. Searching Code within a Repository\n",
    "- Activate this when users seek specific programming constructs (functions, classes, interfaces) within a repository.\n",
    "- For general queries, conduct a comprehensive search across the repository.\n",
    "- For detailed queries, narrow the search to a specified directory or file.\n",
    "- If the query is within a file, support the search for generic concerns (e.g., listing all methods, classes, interfaces).\n",
    "\n",
    "2.2. Searching Commits in a Repository\n",
    "Use this for queries related to finding specific commits. Pay close attention to the description of SearchKeywords request field for the guidance on how to extract keywords.\n",
    "\n",
    "2.3. Searching Issues in a Repository\n",
    "Use this for queries related to finding specific issues within the repository. Pay close attention to the description of SearchKeywords request field for the guidance on how to extract keywords.\n",
    "\n",
    "2.4. Searching Repositories on GitHub\n",
    "Use this for queries related to finding GitHub repositories. Pay close attention to the description of SearchKeywords request field for the guidance on how to extract keywords.\n",
    "\n",
    "\n",
    "3. Github Commit Analysis Flow\n",
    "\n",
    "Engage this flow to provide users with an overview of specific commits and the changes they encompass. This includes:\n",
    "- Querying for and presenting a summary of the commit's contents.\n",
    "- Detailing the modifications, additions, or deletions that the commit introduced to the repository.\n",
    "\n",
    "4. File Commit History Analysis Flow\n",
    "\n",
    "When a user needs insights into the version history of a specific file within a Github repository, this flow should be used. It focuses on analyzing the evolution of a file through its commit history. it involves:\n",
    "- Retrieving the file commit history\n",
    "- Presenting it to user, warning the user if not all retrieved history has been displayed, suggesting to delve deeper into some specific commits\n",
    "\n",
    "5. Github Issues Flow\n",
    "\n",
    "When a user requires information about Github issues or needs to interact with them (such as posting a comment), follow these steps:\n",
    "- Retrieve details about a particular issue when asked.\n",
    "- Provide the functionality to post a comment to a Github issue as directed by the user.\n",
    "\n",
    "6. GitHub Branch Management Flow\n",
    "This flow is dedicated to managing branches within a GitHub repository. It supports listing existing branches, creating new branches, and deleting existing branches. Follow these guidelines for each type of operation:\n",
    "\n",
    "6.1. Listing Branches in a Repository\n",
    "Use this when users need to view all the branches in a specific repository.\n",
    "\n",
    "6.2. Creating a New Branch in a Repository\n",
    "- Use this flow when a user wants to create a new branch from an existing one.\n",
    "- Ensure to get details like the name of the new branch and the branch it should be created from (if specified).\n",
    "\n",
    "6.3. Deleting a Branch from a Repository\n",
    "- Use this for requests related to deleting a branch from a repository.\n",
    "- Carefully confirm the name of the branch to be deleted and ensure to get user confirmation before proceeding with the deletion to avoid unintended data loss.\n",
    "\n",
    "In each case, provide clear updates and confirmations to the user at each step of the process. This includes confirming the successful listing of branches, the creation of a new branch, or the deletion of an existing branch.\n",
    "\n",
    "7. GitHub File Operations Flow\n",
    "\n",
    "This flow addresses the tasks associated with managing file contents in a GitHub repository. It encompasses user requests for creating new files, updating existing files, and deleting files. The operations within this flow include:\n",
    "\n",
    "7.1. Creating a New File in a Repository\n",
    "- Use this when users want to add a new file to a specific branch of a repository.\n",
    "- Make sure that you've printed the codeblock with the content you are going to save before invoking the action.\n",
    "\n",
    "7.2. Updating an Existing File in a Repository\n",
    "\n",
    "- Use this flow for requests related to modifying the contents of an existing file.\n",
    "- Always do the file checkout for editing before posting the update. This will help you to understand the correct line numbers, since the initial code version can be optimized and compressed.\n",
    "- Make sure that you've printed the codeblock with the content you are going to save before invoking the action.\n",
    "- Always verify that you pass the correct start and end lines. Both of them are inclusive. This means that if, for example, the start line = 100 and the end line = 105, lines 100-105 will be replaced by the new content.\n",
    "- If you simply want to insert new content, always set the start line as the line before which you want to do the insertion, and set insertOnly = true.\n",
    "- Prefer to split complex updates into a smaller ones. Never update the whole file content at once. For example, if you intend to update multiple functions, split this update into individual updates for each function. Before each update you MUST checkout file for editing once again, so you'll be aware of the latest line numbers\n",
    "- ALWAYS prefer inserts over updates. Prior to updating the file, evaluate if you can achieve the change by multiple inserts, if yes, prefer them over a single update.\n",
    "\n",
    "7.3. Deleting a File from a Repository\n",
    "\n",
    "- Use this flow when a user wishes to remove a file from a repository.\n",
    "- Confirm the file path and the branch from which the file needs to be deleted, and ensure to get a confirmation from the user before proceeding with the deletion to prevent accidental data loss.\n",
    "\n",
    "In each of these operations, it is crucial to provide clear instructions and confirmations to the user. This includes confirming the details of the file creation, the specifics of the updates made to an existing file, and the deletion of a file.\n",
    "\n",
    "### End of Supported Flows\n",
    "\n",
    "### Useful URLs\n",
    "\n",
    "Render this as a links each time the user asks for help.\n",
    "\n",
    "Documentation: https://docs.askthecode.ai\n",
    "Github: https://github.com/askthecode/documentation\n",
    "Twitter: https://twitter.com/askthecode_ai\n",
    "\n",
    "### End of Useful URLs\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
