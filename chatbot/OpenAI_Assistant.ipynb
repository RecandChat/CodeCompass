{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install the latest version of the openai library\n",
    "!pip install openai -q --upgrade\n",
    "\n",
    "# imports\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OpenAI key from a file\n",
    "with open('../secrets/openAI_key', 'r') as file:\n",
    "    openAI_key = file.read().replace('\\n', '')\n",
    "\n",
    "# load the github token\n",
    "with open('../secrets/github_token', 'r') as file:\n",
    "    github_token = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize the client\n",
    "client = OpenAI(\n",
    "    api_key = openAI_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ASK THE CODE API functions\n",
    "\"\"\"\n",
    "\n",
    "def get_repo_structure(url, branch=None, relativePaths=None):\n",
    "    # URL of Askthecode API endpoint\n",
    "    get_repo_structure_url = \"https://gabriel.askthecode.ai/api/repository/structure\"\n",
    "\n",
    "    # parameters: url(required), branch(optional), relativePaths(optional)\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'branch': branch,\n",
    "        'relativePaths': relativePaths\n",
    "    }\n",
    "\n",
    "    # headers\n",
    "    headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {github_token}\",  # Ensure your token is correctly referenced\n",
    "    \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the post request\n",
    "    response = requests.post(get_repo_structure_url, json=params, headers=headers)\n",
    "\n",
    "    # check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Parsing the response JSON\n",
    "        structure_response = response.json()\n",
    "        # Return the response data instead of printing\n",
    "        return structure_response\n",
    "    else:\n",
    "        # Return a formatted error message\n",
    "        return f\"Failed to get the repository structure: {response.status_code}, Reason: {response.reason}\"\n",
    "\n",
    "\n",
    "# get repo content\n",
    "def get_repo_content(url, filePaths, branch=None, relativePath=None):\n",
    "    # URL of Askthecode API endpoint\n",
    "    get_repo_content_url = \"https://gabriel.askthecode.ai/api/repository/content\"\n",
    "\n",
    "    # parameters: url(required), filePaths(required), branch(optional), relativePath(optional)\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'filePaths': filePaths,\n",
    "        'branch': branch,\n",
    "        'relativePath': relativePath\n",
    "    }\n",
    "\n",
    "    # headers\n",
    "    headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {github_token}\", \n",
    "    \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the post request\n",
    "    response = requests.post(get_repo_content_url, json=params, headers=headers)\n",
    "\n",
    "    # check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Parsing the response JSON\n",
    "        content_response = response.json()\n",
    "        # Return the response data instead of printing\n",
    "        return content_response\n",
    "    \n",
    "    else:\n",
    "       return f\"Failed to get the repository content: {response.status_code}\"\n",
    "\n",
    "# get repo branches\n",
    "def get_repo_branches(url):\n",
    "    # URL of Askthecode API endpoint\n",
    "    get_repo_branches_url = \"https://gabriel.askthecode.ai/api/repository/branch/list\"\n",
    "\n",
    "    # parameters: url(required)\n",
    "    params = {\n",
    "        'url': url\n",
    "    }\n",
    "\n",
    "   # headers\n",
    "    headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {github_token}\", \n",
    "    \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the post request\n",
    "    response = requests.post(get_repo_branches_url, json=params, headers=headers)\n",
    "\n",
    "    # check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Parsing the response JSON\n",
    "        branches_response = response.json()\n",
    "        # Return the response data instead of printing\n",
    "        return branches_response\n",
    "    else:\n",
    "        return f\"Failed to get the repository branches: {response.status_code}\"\n",
    "    \n",
    "# Get commit history\n",
    "def get_commit_history(url, branch=None, filePath=None):\n",
    "    # URL of Askthecode API endpoint\n",
    "    get_commit_history_url = \"https://gabriel.askthecode.ai/api/repository/commit/history\"\n",
    "\n",
    "    # parameters: url(required), branch(optional), filePath(optional)\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'branch': branch,\n",
    "        'filePath': filePath\n",
    "    }\n",
    "\n",
    "    # headers\n",
    "    headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {github_token}\", \n",
    "    \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the post request\n",
    "    response = requests.post(get_commit_history_url, json=params, headers=headers)\n",
    "\n",
    "    # check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Parsing the response JSON\n",
    "        commit_history_response = response.json()\n",
    "        # Return the response data instead of printing\n",
    "        return commit_history_response\n",
    "    else:\n",
    "        return f\"Failed to get the commit history: {response.status_code}\"\n",
    "    \n",
    "# search repo code\n",
    "def search_repo_code(url, searchKeywords, branch=None, relativePath=None, searchHitLinesCount=None, skipMatchesCount=None):\n",
    "    # URL of Askthecode API endpoint\n",
    "    search_repo_code_url = \"https://gabriel.askthecode.ai/api/search/repository/code\"\n",
    "\n",
    "    # parameters: url(required), searchKeywords(required), branch(optional), relativePath(optional), searchHitLinesCount(optional), skipMatchesCount(optional)\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'searchKeywords': searchKeywords,\n",
    "        'branch': branch,\n",
    "        'relativePath': relativePath,\n",
    "        'searchHitLinesCount': searchHitLinesCount,\n",
    "        'skipMatchesCount': skipMatchesCount\n",
    "    }\n",
    "\n",
    "    # headers\n",
    "    headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {github_token}\", \n",
    "    \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the post request\n",
    "    response = requests.post(search_repo_code_url, json=params, headers=headers)\n",
    "\n",
    "    # check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Parsing the response JSON\n",
    "        search_code_response = response.json()\n",
    "        # Return the response data instead of printing\n",
    "        return search_code_response\n",
    "    else:\n",
    "        return f\"Failed to search the repository code: {response.status_code}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the structure of the repository:\n",
      "{'branchName': 'main', 'files': ['.devcontainer/Dockerfile', '.devcontainer/devcontainer.json', '.github/.codecov.yml', '.github/CODEOWNERS', '.github/ISSUE_TEMPLATE.md', '.github/ISSUE_TEMPLATE/bug_report.md', '.github/ISSUE_TEMPLATE/feature_request.md', '.github/ISSUE_TEMPLATE/general-ask.md', '.github/PULL_REQUEST_TEMPLATE.md', '.github/actions/azureml-test/action.yml', '.github/actions/get-test-groups/action.yml', '.github/workflows/azureml-cpu-nightly.yml', '.github/workflows/azureml-gpu-nightly.yml', '.github/workflows/azureml-release-pipeline.yml', '.github/workflows/azureml-spark-nightly.yml', '.github/workflows/azureml-unit-tests.yml', '.github/workflows/sarplus.yml', '.github/workflows/update_documentation.yml', 'AUTHORS.md', 'CODE_OF_CONDUCT.md', 'CONTRIBUTING.md', 'GLOSSARY.md', 'LICENSE', 'MANIFEST.in', 'NEWS.md', 'README.md', 'SECURITY.md', 'SETUP.md', 'contrib/README.md', 'contrib/azureml_designer_modules/README.md', 'contrib/azureml_designer_modules/entries/map_entry.py', 'contrib/azureml_designer_modules/entries/ndcg_entry.py', 'contrib/azureml_designer_modules/entries/precision_at_k_entry.py', 'contrib/azureml_designer_modules/entries/recall_at_k_entry.py', 'contrib/azureml_designer_modules/entries/score_sar_entry.py', 'contrib/azureml_designer_modules/entries/stratified_splitter_entry.py', 'contrib/azureml_designer_modules/entries/train_sar_entry.py', 'contrib/azureml_designer_modules/module_specs/map.yaml', 'contrib/azureml_designer_modules/module_specs/ndcg.yaml', 'contrib/azureml_designer_modules/module_specs/precision_at_k.yaml', 'contrib/azureml_designer_modules/module_specs/recall_at_k.yaml', 'contrib/azureml_designer_modules/module_specs/sar_conda.yaml', 'contrib/azureml_designer_modules/module_specs/sar_score.yaml', 'contrib/azureml_designer_modules/module_specs/sar_train.yaml', 'contrib/azureml_designer_modules/module_specs/stratified_splitter.yaml', 'contrib/sarplus/DEVELOPMENT.md', 'contrib/sarplus/README.md', 'contrib/sarplus/VERSION', 'contrib/sarplus/python/.flake8', 'contrib/sarplus/python/README.md', 'contrib/sarplus/python/pyproject.toml', 'contrib/sarplus/python/pysarplus/SARModel.py', 'contrib/sarplus/python/pysarplus/SARPlus.py', 'contrib/sarplus/python/pysarplus/__init__.py', 'contrib/sarplus/python/setup.py', 'contrib/sarplus/python/src/pysarplus.cpp', 'contrib/sarplus/python/tests/conftest.py', 'contrib/sarplus/python/tests/sample-input.txt', 'contrib/sarplus/python/tests/test_pyspark_sar.py', 'contrib/sarplus/scala/build.sbt', 'contrib/sarplus/scala/compat/src/main/scala/com/microsoft/sarplus/compat/spark/since3p2defvisible.scala', 'contrib/sarplus/scala/project/Utils.scala', 'contrib/sarplus/scala/project/build.properties', 'contrib/sarplus/scala/project/plugins.sbt', 'contrib/sarplus/scala/python/pysarplus_dummy/__init__.py', 'contrib/sarplus/scala/python/setup.py', 'contrib/sarplus/scala/src/main/scala/com/microsoft/sarplus/DefaultSource.scala', 'contrib/sarplus/scala/src/main/scala/com/microsoft/sarplus/SARCacheOutputWriter.scala', 'contrib/sarplus/scala/src/main/scala/com/microsoft/sarplus/SARCacheOutputWriterFactory.scala', 'contrib/sarplus/scala/src/test/scala/com/microsoft/sarplus/SARCacheOutputWriterSpec.scala', 'docs/_config.yml', 'docs/_toc.yml', 'docs/datasets.rst', 'docs/evaluation.rst', 'docs/intro.md', 'docs/models.rst', 'docs/requirements-doc.txt', 'docs/tuning.rst', 'docs/utils.rst', 'examples/00_quick_start/README.md', 'examples/00_quick_start/als_movielens.ipynb', 'examples/00_quick_start/dkn_MIND.ipynb', 'examples/00_quick_start/fastai_movielens.ipynb', 'examples/00_quick_start/geoimc_movielens.ipynb', 'examples/00_quick_start/lightgbm_tinycriteo.ipynb', 'examples/00_quick_start/lstur_MIND.ipynb', 'examples/00_quick_start/naml_MIND.ipynb', 'examples/00_quick_start/ncf_movielens.ipynb', 'examples/00_quick_start/npa_MIND.ipynb', 'examples/00_quick_start/nrms_MIND.ipynb', 'examples/00_quick_start/rbm_movielens.ipynb', 'examples/00_quick_start/rlrmc_movielens.ipynb', 'examples/00_quick_start/sar_movielens.ipynb', 'examples/00_quick_start/sar_movielens_with_azureml.ipynb', 'examples/00_quick_start/sar_movieratings_with_azureml_designer.ipynb', 'examples/00_quick_start/sasrec_amazon.ipynb', 'examples/00_quick_start/sequential_recsys_amazondataset.ipynb', 'examples/00_quick_start/tfidf_covid.ipynb', 'examples/00_quick_start/wide_deep_movielens.ipynb', 'examples/00_quick_start/xdeepfm_criteo.ipynb', 'examples/01_prepare_data/README.md', 'examples/01_prepare_data/data_split.ipynb', 'examples/01_prepare_data/data_transform.ipynb', 'examples/01_prepare_data/mind_utils.ipynb', 'examples/01_prepare_data/wikidata_knowledge_graph.ipynb', 'examples/02_model_collaborative_filtering/README.md', 'examples/02_model_collaborative_filtering/als_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/baseline_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/fm_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/lightfm_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/rbm_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/sar_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb', 'examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb', 'examples/02_model_content_based_filtering/README.md', 'examples/02_model_content_based_filtering/dkn_deep_dive.ipynb', 'examples/02_model_content_based_filtering/mmlspark_lightgbm_criteo.ipynb', 'examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb', 'examples/03_evaluate/README.md', 'examples/03_evaluate/als_movielens_diversity_metrics.ipynb', 'examples/03_evaluate/evaluation.ipynb', 'examples/04_model_select_and_optimize/README.md', 'examples/04_model_select_and_optimize/azureml_hyperdrive_surprise_svd.ipynb', 'examples/04_model_select_and_optimize/azureml_hyperdrive_wide_and_deep.ipynb', 'examples/04_model_select_and_optimize/nni_ncf.ipynb', 'examples/04_model_select_and_optimize/nni_surprise_svd.ipynb', 'examples/04_model_select_and_optimize/train_scripts/svd_training.py', 'examples/04_model_select_and_optimize/train_scripts/wide_deep_training.py', 'examples/04_model_select_and_optimize/tuning_spark_als.ipynb', 'examples/05_operationalize/README.md', 'examples/05_operationalize/aks_locust_load_test.ipynb', 'examples/05_operationalize/als_movie_o16n.ipynb', 'examples/05_operationalize/lightgbm_criteo_o16n.ipynb', 'examples/06_benchmarks/README.md', 'examples/06_benchmarks/benchmark_utils.py', 'examples/06_benchmarks/movielens.ipynb', 'examples/07_tutorials/KDD2020-tutorial/README.md', 'examples/07_tutorials/KDD2020-tutorial/dkn.yaml', 'examples/07_tutorials/KDD2020-tutorial/lightgcn.yaml', 'examples/07_tutorials/KDD2020-tutorial/pandas-subgraph-local-samples.ipynb', 'examples/07_tutorials/KDD2020-tutorial/reco_cpu_kdd.yaml', 'examples/07_tutorials/KDD2020-tutorial/reco_gpu_kdd.yaml', 'examples/07_tutorials/KDD2020-tutorial/run_transE.sh', 'examples/07_tutorials/KDD2020-tutorial/step1_data_preparation.ipynb', 'examples/07_tutorials/KDD2020-tutorial/step2_pretraining-embeddings.ipynb', 'examples/07_tutorials/KDD2020-tutorial/step3_run_dkn.ipynb', 'examples/07_tutorials/KDD2020-tutorial/step4_run_dkn_item2item.ipynb', 'examples/07_tutorials/KDD2020-tutorial/step5_run_lightgcn.ipynb', 'examples/07_tutorials/KDD2020-tutorial/utils/PandasMagClass.py', 'examples/07_tutorials/KDD2020-tutorial/utils/data_helper.py', 'examples/07_tutorials/KDD2020-tutorial/utils/general.py', 'examples/07_tutorials/KDD2020-tutorial/utils/task_helper.py', 'examples/README.md', 'examples/run_notebook_on_azureml.ipynb', 'examples/template.ipynb', 'pyproject.toml', 'recommenders/README.md', 'recommenders/__init__.py', 'recommenders/datasets/__init__.py', 'recommenders/datasets/amazon_reviews.py', 'recommenders/datasets/cosmos_cli.py', 'recommenders/datasets/covid_utils.py', 'recommenders/datasets/criteo.py', 'recommenders/datasets/download_utils.py', 'recommenders/datasets/mind.py', 'recommenders/datasets/movielens.py', 'recommenders/datasets/pandas_df_utils.py', 'recommenders/datasets/python_splitters.py', 'recommenders/datasets/spark_splitters.py', 'recommenders/datasets/sparse.py', 'recommenders/datasets/split_utils.py', 'recommenders/datasets/wikidata.py', 'recommenders/evaluation/__init__.py', 'recommenders/evaluation/python_evaluation.py', 'recommenders/evaluation/spark_evaluation.py', 'recommenders/models/__init__.py', 'recommenders/models/cornac/__init__.py', 'recommenders/models/cornac/cornac_utils.py', 'recommenders/models/deeprec/DataModel/ImplicitCF.py', 'recommenders/models/deeprec/DataModel/__init__.py', 'recommenders/models/deeprec/__init__.py', 'recommenders/models/deeprec/config/asvd.yaml', 'recommenders/models/deeprec/config/caser.yaml', 'recommenders/models/deeprec/config/gru.yaml', 'recommenders/models/deeprec/config/lightgcn.yaml', 'recommenders/models/deeprec/config/nextitnet.yaml', 'recommenders/models/deeprec/config/sli_rec.yaml', 'recommenders/models/deeprec/config/sum.yaml', 'recommenders/models/deeprec/deeprec_utils.py', 'recommenders/models/deeprec/io/__init__.py', 'recommenders/models/deeprec/io/dkn_item2item_iterator.py', 'recommenders/models/deeprec/io/dkn_iterator.py', 'recommenders/models/deeprec/io/iterator.py', 'recommenders/models/deeprec/io/nextitnet_iterator.py', 'recommenders/models/deeprec/io/sequential_iterator.py', 'recommenders/models/deeprec/models/__init__.py', 'recommenders/models/deeprec/models/base_model.py', 'recommenders/models/deeprec/models/dkn.py', 'recommenders/models/deeprec/models/dkn_item2item.py', 'recommenders/models/deeprec/models/graphrec/__init__.py', 'recommenders/models/deeprec/models/graphrec/lightgcn.py', 'recommenders/models/deeprec/models/sequential/__init__.py', 'recommenders/models/deeprec/models/sequential/asvd.py', 'recommenders/models/deeprec/models/sequential/caser.py', 'recommenders/models/deeprec/models/sequential/gru.py', 'recommenders/models/deeprec/models/sequential/nextitnet.py', 'recommenders/models/deeprec/models/sequential/rnn_cell_implement.py', 'recommenders/models/deeprec/models/sequential/sequential_base_model.py', 'recommenders/models/deeprec/models/sequential/sli_rec.py', 'recommenders/models/deeprec/models/sequential/sum.py', 'recommenders/models/deeprec/models/sequential/sum_cells.py', 'recommenders/models/deeprec/models/xDeepFM.py', 'recommenders/models/fastai/__init__.py', 'recommenders/models/fastai/fastai_utils.py', 'recommenders/models/geoimc/__init__.py', 'recommenders/models/geoimc/geoimc_algorithm.py', 'recommenders/models/geoimc/geoimc_data.py', 'recommenders/models/geoimc/geoimc_predict.py', 'recommenders/models/geoimc/geoimc_utils.py', 'recommenders/models/lightfm/__init__.py', 'recommenders/models/lightfm/lightfm_utils.py', 'recommenders/models/lightgbm/__init__.py', 'recommenders/models/lightgbm/lightgbm_utils.py', 'recommenders/models/ncf/__init__.py', 'recommenders/models/ncf/dataset.py', 'recommenders/models/ncf/ncf_singlenode.py', 'recommenders/models/newsrec/__init__.py', 'recommenders/models/newsrec/io/__init__.py', 'recommenders/models/newsrec/io/mind_all_iterator.py', 'recommenders/models/newsrec/io/mind_iterator.py', 'recommenders/models/newsrec/models/__init__.py', 'recommenders/models/newsrec/models/base_model.py', 'recommenders/models/newsrec/models/layers.py', 'recommenders/models/newsrec/models/lstur.py', 'recommenders/models/newsrec/models/naml.py', 'recommenders/models/newsrec/models/npa.py', 'recommenders/models/newsrec/models/nrms.py', 'recommenders/models/newsrec/newsrec_utils.py', 'recommenders/models/rbm/__init__.py', 'recommenders/models/rbm/rbm.py', 'recommenders/models/rlrmc/RLRMCalgorithm.py', 'recommenders/models/rlrmc/RLRMCdataset.py', 'recommenders/models/rlrmc/__init__.py', 'recommenders/models/rlrmc/conjugate_gradient_ms.py', 'recommenders/models/sar/__init__.py', 'recommenders/models/sar/sar_singlenode.py', 'recommenders/models/sasrec/__init__.py', 'recommenders/models/sasrec/model.py', 'recommenders/models/sasrec/sampler.py', 'recommenders/models/sasrec/ssept.py', 'recommenders/models/sasrec/util.py', 'recommenders/models/surprise/__init__.py', 'recommenders/models/surprise/surprise_utils.py', 'recommenders/models/tfidf/__init__.py', 'recommenders/models/tfidf/tfidf_utils.py', 'recommenders/models/vae/__init__.py', 'recommenders/models/vae/multinomial_vae.py', 'recommenders/models/vae/standard_vae.py', 'recommenders/models/vowpal_wabbit/__init__.py', 'recommenders/models/vowpal_wabbit/vw.py', 'recommenders/models/wide_deep/__init__.py', 'recommenders/models/wide_deep/wide_deep_utils.py', 'recommenders/tuning/__init__.py', 'recommenders/tuning/nni/__init__.py', 'recommenders/tuning/nni/ncf_training.py', 'recommenders/tuning/nni/ncf_utils.py', 'recommenders/tuning/nni/nni_utils.py', 'recommenders/tuning/nni/svd_training.py', 'recommenders/tuning/parameter_sweep.py', 'recommenders/utils/__init__.py', 'recommenders/utils/constants.py', 'recommenders/utils/general_utils.py', 'recommenders/utils/gpu_utils.py', 'recommenders/utils/k8s_utils.py', 'recommenders/utils/notebook_memory_management.py', 'recommenders/utils/notebook_utils.py', 'recommenders/utils/plot.py', 'recommenders/utils/python_utils.py', 'recommenders/utils/spark_utils.py', 'recommenders/utils/tf_utils.py', 'recommenders/utils/timer.py', 'scenarios/README.md', 'scenarios/ads/README.md', 'scenarios/food_and_restaurants/README.md', 'scenarios/gaming/README.md', 'scenarios/news/README.md', 'scenarios/retail/README.md', 'scenarios/travel/README.md', 'setup.py', 'tests/README.md', 'tests/__init__.py', 'tests/ci/README.md', 'tests/ci/__init__.py', 'tests/ci/azureml_tests/__init__.py', 'tests/ci/azureml_tests/run_groupwise_pytest.py', 'tests/ci/azureml_tests/submit_groupwise_azureml_pytest.py', 'tests/ci/azureml_tests/test_groups.py', 'tests/conftest.py', 'tests/data_validation/__init__.py', 'tests/data_validation/examples/__init__.py', 'tests/data_validation/examples/test_mind.py', 'tests/data_validation/examples/test_wikidata.py', 'tests/data_validation/recommenders/__init__.py', 'tests/data_validation/recommenders/datasets/__init__.py', 'tests/data_validation/recommenders/datasets/test_covid_utils.py', 'tests/data_validation/recommenders/datasets/test_criteo.py', 'tests/data_validation/recommenders/datasets/test_mind.py', 'tests/data_validation/recommenders/datasets/test_movielens.py', 'tests/data_validation/recommenders/datasets/test_wikidata.py', 'tests/functional/__init__.py', 'tests/functional/examples/__init__.py', 'tests/functional/examples/test_notebooks_gpu.py', 'tests/functional/examples/test_notebooks_pyspark.py', 'tests/functional/examples/test_notebooks_python.py', 'tests/integration/__init__.py', 'tests/integration/recommenders/__init__.py', 'tests/integration/recommenders/utils/__init__.py', 'tests/integration/recommenders/utils/test_k8s_utils.py', 'tests/performance/__init__.py', 'tests/performance/recommenders/__init__.py', 'tests/performance/recommenders/evaluation/__init__.py', 'tests/performance/recommenders/evaluation/test_python_evaluation_time_performance.py', 'tests/regression/__init__.py', 'tests/regression/test_compatibility_tf.py', 'tests/responsible_ai/__init__.py', 'tests/responsible_ai/recommenders/__init__.py', 'tests/responsible_ai/recommenders/datasets/__init__.py', 'tests/responsible_ai/recommenders/datasets/test_criteo_privacy.py', 'tests/responsible_ai/recommenders/datasets/test_movielens_privacy.py', 'tests/security/__init__.py', 'tests/security/test_dependency_security.py', 'tests/smoke/__init__.py', 'tests/smoke/examples/__init__.py', 'tests/smoke/examples/test_notebooks_gpu.py', 'tests/smoke/examples/test_notebooks_pyspark.py', 'tests/smoke/examples/test_notebooks_python.py', 'tests/smoke/recommenders/__init__.py', 'tests/smoke/recommenders/recommender/__init__.py', 'tests/smoke/recommenders/recommender/test_deeprec_model.py', 'tests/smoke/recommenders/recommender/test_deeprec_utils.py', 'tests/smoke/recommenders/recommender/test_newsrec_model.py', 'tests/smoke/recommenders/recommender/test_newsrec_utils.py', 'tests/unit/__init__.py', 'tests/unit/examples/__init__.py', 'tests/unit/examples/test_notebooks_gpu.py', 'tests/unit/examples/test_notebooks_pyspark.py', 'tests/unit/examples/test_notebooks_python.py', 'tests/unit/recommenders/__init__.py', 'tests/unit/recommenders/datasets/__init__.py', 'tests/unit/recommenders/datasets/test_download_utils.py', 'tests/unit/recommenders/datasets/test_pandas_df_utils.py', 'tests/unit/recommenders/datasets/test_python_splitter.py', 'tests/unit/recommenders/datasets/test_spark_splitter.py', 'tests/unit/recommenders/datasets/test_sparse.py', 'tests/unit/recommenders/evaluation/__init__.py', 'tests/unit/recommenders/evaluation/conftest.py', 'tests/unit/recommenders/evaluation/test_python_evaluation.py', 'tests/unit/recommenders/evaluation/test_spark_evaluation.py', 'tests/unit/recommenders/models/__init__.py', 'tests/unit/recommenders/models/test_cornac_utils.py', 'tests/unit/recommenders/models/test_deeprec_model.py', 'tests/unit/recommenders/models/test_deeprec_utils.py', 'tests/unit/recommenders/models/test_geoimc.py', 'tests/unit/recommenders/models/test_lightfm_utils.py', 'tests/unit/recommenders/models/test_ncf_dataset.py', 'tests/unit/recommenders/models/test_ncf_singlenode.py', 'tests/unit/recommenders/models/test_newsrec_model.py', 'tests/unit/recommenders/models/test_newsrec_utils.py', 'tests/unit/recommenders/models/test_rbm.py', 'tests/unit/recommenders/models/test_sar_singlenode.py', 'tests/unit/recommenders/models/test_sasrec_model.py', 'tests/unit/recommenders/models/test_surprise_utils.py', 'tests/unit/recommenders/models/test_tfidf_utils.py', 'tests/unit/recommenders/models/test_vowpal_wabbit.py', 'tests/unit/recommenders/models/test_wide_deep_utils.py', 'tests/unit/recommenders/tuning/__init__.py', 'tests/unit/recommenders/tuning/test_ncf_utils.py', 'tests/unit/recommenders/tuning/test_nni_utils.py', 'tests/unit/recommenders/tuning/test_sweep.py', 'tests/unit/recommenders/utils/__init__.py', 'tests/unit/recommenders/utils/programmatic_notebook_execution.ipynb', 'tests/unit/recommenders/utils/test_general_utils.py', 'tests/unit/recommenders/utils/test_gpu_utils.py', 'tests/unit/recommenders/utils/test_notebook_utils.ipynb', 'tests/unit/recommenders/utils/test_notebook_utils.py', 'tests/unit/recommenders/utils/test_plot.py', 'tests/unit/recommenders/utils/test_python_utils.py', 'tests/unit/recommenders/utils/test_tf_utils.py', 'tests/unit/recommenders/utils/test_timer.py', 'tools/__init__.py', 'tools/databricks_install.py', 'tools/docker/Dockerfile', 'tools/docker/README.md'], 'assistantNextSteps': '1. Thoroughly analyze the repository structure and try to make some assumptions on the content of each file.2. Request contents of no more than 20 files. Order them by relevance descending. ', 'usefulUrls': {'websiteUrl': 'https://askthecode.ai', 'documentationUrl': 'https://docs.askthecode.ai', 'githubUrl': 'https://github.com/askthecode/documentation', 'twitterUrl': 'https://twitter.com/askthecode_ai'}}\n",
      "Here is the content of the repository:\n",
      "{'branchName': 'main', 'files': [{'path': '.github/.codecov.yml', 'content': '# This file controls how codecov submit comments in the PR about code coverage.\\n# For more details, please see: \\n# https://docs.codecov.com/docs/pull-request-comments#section-behavior\\ncomment:\\n  behavior: default\\n\\nflags:\\n  nightly:\\n    joined: false'}], 'assistantNextSteps': '1. Analyze if the answer to the user question is contained in the returned files. 2. If yes, print the response to the user. If not, try to request the contents of another files.', 'usefulUrls': {'websiteUrl': 'https://askthecode.ai', 'documentationUrl': 'https://docs.askthecode.ai', 'githubUrl': 'https://github.com/askthecode/documentation', 'twitterUrl': 'https://twitter.com/askthecode_ai'}}\n",
      "Here are the branches of the repository:\n",
      "{'branches': [{'name': '42-external-data-storage', 'headSha': 'a2643d0a9cee45d2475bb11ecb558de95fd5b061'}, {'name': 'chatbot', 'headSha': '80b7e549a9ce4d54f53088d51bb6931e0d1a0031'}, {'name': 'main', 'headSha': '87dcdbf196b78cf208b80a79973a9d69d763dee6'}, {'name': 'mlops', 'headSha': 'a194bec38218b170253532e9ca76f5b770f27937'}], 'assistantNextSteps': \"1. Respond with the list of branches. If not specified by user, render them as the list, where you'll display the branch name and the SHA of last commit. 2. Say to user that he can request the commit details for the last commit of each branch.\", 'usefulUrls': {'websiteUrl': 'https://askthecode.ai', 'documentationUrl': 'https://docs.askthecode.ai', 'githubUrl': 'https://github.com/askthecode/documentation', 'twitterUrl': 'https://twitter.com/askthecode_ai'}}\n",
      "Here is the commit history of the repository:\n",
      "{'filePath': '.devcontainer/devcontainer.json', 'commits': [{'commitUrl': 'https://github.com/recommenders-team/recommenders/commit/846d214476bfb1f4e6898503bebb6a33a14db410', 'message': 'Adding codespace deployment (#1521)\\n\\n* adding codespace configuration\\r\\n\\r\\n* adding codespace configuration and dockerfile\\r\\n\\r\\n* fix java installation in codespace docker\\r\\n\\r\\n* adding installation for  package option\\r\\n\\r\\n* updating packages installed\\r\\n\\r\\n* updating ipykernel version and port forwarded for codespace\\r\\n\\r\\n* updating pip install and path\\r\\n\\r\\n* reverting postcreatecommand changes\\r\\n\\r\\n* using root for codespace\\r\\n\\r\\n* updating jupyter notebook setup and trying non-root user\\r\\n\\r\\n* fixes for using non-root user\\r\\n\\r\\n* allowing editable install into user by disabling pep517 for pip\\r\\n\\r\\n* allow user editable pip installs with user flag\\r\\n\\r\\n* removing duplicate env\\r\\n\\r\\n* adding port 4040 for spark monitoring\\r\\n\\r\\n* removing 4040 port fwd', 'commitDate': '2021-10-08T16:52:51', 'author': 'gramhagen'}], 'count': 1, 'assistantNextSteps': \"1. Print that you've successfully retrieved 1 commits from the file history. 2. Print the response to the user. 3. If you decide to print not all commits that were returned to you, notify user about that and ask if he wants to see others.  4. Notify user that you've retrieved all commits from the file history.\", 'usefulUrls': {'websiteUrl': 'https://askthecode.ai', 'documentationUrl': 'https://docs.askthecode.ai', 'githubUrl': 'https://github.com/askthecode/documentation', 'twitterUrl': 'https://twitter.com/askthecode_ai'}}\n",
      "Here is the search result:\n",
      "{'branchName': 'main', 'searchResults': [{'path': 'recommenders/models/newsrec/models/npa.py', 'matches': ['    \"\"\"NPA model(Neural News Recommendation with Attentive Multi-View Learning)\\n\\n    Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang and Xing Xie:\\n    NPA: Neural News Recommendation with Personalized Attention, KDD 2019, ADS track.\\n\\n    Attributes:\\n        word2vec_embedding (numpy.ndarray): Pretrained word embedding matrix.\\n']}, {'path': 'recommenders/models/newsrec/models/nrms.py', 'matches': ['\\n\\nclass NRMSModel(BaseModel):\\n    \"\"\"NRMS model(Neural News Recommendation with Multi-Head Self-Attention)\\n\\n    Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang,and Xing Xie, \"Neural News\\n    Recommendation with Multi-Head Self-Attention\" in Proceedings of the 2019 Conference\\n']}, {'path': 'README.md', 'matches': ['| Multinomial VAE | Collaborative Filtering | Generative model for predicting user/item interactions. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb) |\\n| Neural Recommendation with Long- and Short-term User Representations (LSTUR)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with long- and short-term user interest modeling. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/lstur_MIND.ipynb) |\\n| Neural Recommendation with Attentive Multi-View Learning (NAML)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with attentive multi-view learning. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/naml_MIND.ipynb) |\\n', '| Neural Collaborative Filtering (NCF) | Collaborative Filtering | Deep learning algorithm with enhanced performance for user/item implicit feedback. It works in the CPU/GPU environment.| [Quick start](examples/00_quick_start/ncf_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) |\\n| Neural Recommendation with Personalized Attention (NPA)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with personalized attention network. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/npa_MIND.ipynb) |\\n| Neural Recommendation with Multi-Head Self-Attention (NRMS)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with multi-head self-attention. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/nrms_MIND.ipynb) |\\n']}, {'path': 'examples/00_quick_start/npa_MIND.ipynb', 'matches': ['            \"cell_type\": \"markdown\",\\n            \"metadata\": {},\\n            \"source\": [\\n                \"# NPA: Neural News Recommendation with Personalized Attention\\\\n\",\\n                \"NPA \\\\\\\\[1\\\\\\\\] is a news recommendation model with personalized attention. The core of NPA is a news representation model and a user representation model. In the news representation model we use a CNN network to learn hidden representations of news articles based on their titles. In the user representation model we learn the representations of users based on the representations of their clicked news articles. In addition, a word-level and a news-level personalized attention are used to capture different informativeness for different users.\\\\n\",\\n                \"\\\\n\",\\n                \"## Properties of NPA:\\\\n\",\\n']}, {'path': 'examples/00_quick_start/naml_MIND.ipynb', 'matches': ['            \"source\": [\\n                \"# NAML: Neural News Recommendation with Attentive Multi-View Learning\\\\n\",\\n                \"NAML \\\\\\\\[1\\\\\\\\] is a multi-view news recommendation approach. The core of NAML is a news encoder and a user encoder. The newsencoder is composed of a title encoder, a body encoder, a vert encoder and a subvert encoder. The CNN-based title encoder and body encoder learn title and body representations by capturing words semantic information. After getting news title, body, vert and subvert representations, an attention network is used to aggregate those vectors. In the user encoder, we learn representations of users from their browsed news. Besides, we apply additive attention to learn more informative news and user representations by selecting important words and news.\\\\n\",\\n', '                \"## Properties of NAML:\\\\n\",\\n                \"- NAML is a multi-view neural news recommendation approach.\\\\n\",\\n                \"- It uses news title, news body, news vert and news subvert to get news repersentations. And it uses user historical behaviors to learn user representations.\\\\n\",\\n']}, {'path': 'examples/00_quick_start/nrms_MIND.ipynb', 'matches': ['            \"source\": [\\n                \"# NRMS: Neural News Recommendation with Multi-Head Self-Attention\\\\n\",\\n                \"NRMS \\\\\\\\[1\\\\\\\\] is a neural news recommendation approach with multi-head selfattention. The core of NRMS is a news encoder and a user encoder. In the newsencoder, a multi-head self-attentions is used to learn news representations from news titles by modeling the interactions between words. In the user encoder, we learn representations of users from their browsed news and use multihead self-attention to capture the relatedness between the news. Besides, we apply additive\\\\n\",\\n', '                \"## Reference\\\\n\",\\n                \"\\\\\\\\[1\\\\\\\\] Wu et al. \\\\\"Neural News Recommendation with Multi-Head Self-Attention.\\\\\" in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<br>\\\\n\",\\n                \"\\\\\\\\[2\\\\\\\\] Wu, Fangzhao, et al. \\\\\"MIND: A Large-scale Dataset for News Recommendation\\\\\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html <br>\\\\n\",\\n']}, {'path': 'examples/00_quick_start/lstur_MIND.ipynb', 'matches': ['            \"cell_type\": \"markdown\",\\n            \"metadata\": {},\\n            \"source\": [\\n                \"# LSTUR: Neural News Recommendation with Long- and Short-term User Representations\\\\n\",\\n                \"LSTUR \\\\\\\\[1\\\\\\\\] is a news recommendation approach capturing users\\' both long-term preferences and short-term interests. The core of LSTUR is a news encoder and a user encoder.  In the news encoder, we learn representations of news from their titles. In user encoder, we propose to learn long-term\\\\n\",\\n                \"user representations from the embeddings of their IDs. In addition, we propose to learn short-term user representations from their recently browsed news via GRU network. Besides, we propose two methods to combine\\\\n\",\\n                \"long-term and short-term user representations. The first one is using the long-term user representation to initialize the hidden state of the GRU network in short-term user representation. The second one is concatenating both\\\\n\",\\n']}, {'path': 'examples/00_quick_start/dkn_MIND.ipynb', 'matches': ['            \"source\": [\\n                \"# DKN : Deep Knowledge-Aware Network for News Recommendation\\\\n\",\\n                \"\\\\n\",\\n                \"DKN \\\\\\\\[1\\\\\\\\] is a deep learning model which incorporates information from knowledge graph for better news recommendation. Specifically, DKN uses TransX \\\\\\\\[2\\\\\\\\] method for knowledge graph representation learning, then applies a CNN framework, named KCNN, to combine entity embedding with word embedding and generate a final embedding vector for a news article. CTR prediction is made via an attention-based neural scorer. \\\\n\",\\n                \"\\\\n\",\\n                \"## Properties of DKN:\\\\n\",\\n                \"\\\\n\",\\n']}, {'path': 'recommenders/models/newsrec/models/naml.py', 'matches': ['    \"\"\"NAML model(Neural News Recommendation with Attentive Multi-View Learning)\\n\\n    Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang and Xing Xie,\\n    Neural News Recommendation with Attentive Multi-View Learning, IJCAI 2019\\n\\n    Attributes:\\n        word2vec_embedding (numpy.ndarray): Pretrained word embedding matrix.\\n']}, {'path': 'recommenders/models/newsrec/models/lstur.py', 'matches': ['    \"\"\"LSTUR model(Neural News Recommendation with Multi-Head Self-Attention)\\n\\n    Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu and Xing Xie:\\n    Neural News Recommendation with Long- and Short-term User Representations, ACL 2019\\n\\n    Attributes:\\n        word2vec_embedding (numpy.ndarray): Pretrained word embedding matrix.\\n']}, {'path': 'recommenders/README.md', 'matches': ['* LightGBM\\n* NCF\\n* NewsRec\\n  * Neural Recommendation with Long- and Short-term User Representations (LSTUR)\\n  * Neural Recommendation with Attentive Multi-View Learning (NAML)\\n  * Neural Recommendation with Personalized Attention (NPA)\\n  * Neural Recommendation with Multi-Head Self-Attention (NRMS)\\n']}, {'path': 'examples/02_model_content_based_filtering/dkn_deep_dive.ipynb', 'matches': ['            \"source\": [\\n                \"# DKN : Deep Knowledge-Aware Network for News Recommendation\\\\n\",\\n                \"\\\\n\",\\n                \"DKN \\\\\\\\[1\\\\\\\\] is a deep learning model which incorporates information from knowledge graph for better news recommendation. Specifically, DKN uses TransX \\\\\\\\[2\\\\\\\\] method for knowledge graph representation learning, then applies a CNN framework, named KCNN, to combine entity embedding with word embedding and generate a final embedding vector for a news article. CTR prediction is made via an attention-based neural scorer. \\\\n\",\\n                \"\\\\n\",\\n                \"## Properties of DKN:\\\\n\",\\n                \"\\\\n\",\\n']}, {'path': 'examples/00_quick_start/README.md', 'matches': ['[4] _Restricted Boltzmann Machines for Collaborative Filtering_, Ruslan Salakhutdinov, Andriy Mnih and Geoffrey Hinton. ICML 2007.<br>\\n[5] _Wide & Deep Learning for Recommender Systems_, Heng-Tze Cheng et al., arXiv:1606.07792 2016. <br>\\n[6] _A unified framework for structured low-rank matrix learning_, Pratik Jawanpuria and Bamdev Mishra, In International Conference on Machine Learning, 2018. <br>\\n[7] _NAML: Neural News Recommendation with Attentive Multi-View Learning_, Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang and Xing Xie. IJCAI 2019.<br>\\n[8] _NRMS: Neural News Recommendation with Multi-Head Self-Attention_, Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, Xing Xie. in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).<br>\\n[9] _LSTUR: Neural News Recommendation with Long- and Short-term User Representations_, Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu and Xing Xie. ACL 2019.<br>\\n[10] _NPA: Neural News Recommendation with Personalized Attention_, Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang and Xing Xie. KDD 2019, ADS track.<br>\\n']}, {'path': 'examples/07_tutorials/KDD2020-tutorial/step3_run_dkn.ipynb', 'matches': ['            \"metadata\": {},\\n            \"source\": [\\n                \"# DKN : Deep Knowledge-Aware Network for News Recommendation\\\\n\",\\n                \"DKN \\\\\\\\[1\\\\\\\\] is a deep learning model which incorporates information from knowledge graph for better news recommendation. Specifically, DKN uses TransX \\\\\\\\[2\\\\\\\\] method for knowledge graph representaion learning, then applies a CNN framework, named KCNN, to combine entity embedding with word embedding and generate a final embedding vector for a news article. CTR prediction is made via an attention-based neural scorer. \\\\n\",\\n                \"<img src=\\\\\"https://recodatasets.z20.web.core.windows.net/kdd2020/images%2FDKN-introduction-pic.JPG\\\\\" width=\\\\\"600\\\\\">\\\\n\",\\n                \"\\\\n\",\\n                \"## Properties of DKN:\\\\n\",\\n']}, {'path': 'examples/07_tutorials/KDD2020-tutorial/step2_pretraining-embeddings.ipynb', 'matches': ['                \"## Reference\\\\n\",\\n                \"\\\\\\\\[1\\\\\\\\] Wang, Hongwei, et al. \\\\\"DKN: Deep Knowledge-Aware Network for News Recommendation.\\\\\" Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2018.<br>\\\\n\",\\n                \"\\\\\\\\[2\\\\\\\\] Knowledge Graph Embeddings including TransE, TransH, TransR and PTransE. https://github.com/thunlp/KB2E <br>\\\\n\",\\n', '                \"\\\\\\\\[3\\\\\\\\] GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/projects/glove/ <br>\\\\n\",\\n                \"\\\\\\\\[4\\\\\\\\] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (NIPS’13). Curran Associates Inc., Red Hook, NY, USA, 3111–3119. <br>\\\\n\",\\n                \"\\\\\\\\[5\\\\\\\\] Gensim  Word2vec embeddings : https://radimrehurek.com/gensim/models/word2vec.html <br>\"\\n']}], 'assistantNextSteps': '1. Analyze the response and returned matches.2. If they answer users question, print the response. If not, try to either search with different keywords, or try to get the full file contents.', 'usefulUrls': {'websiteUrl': 'https://askthecode.ai', 'documentationUrl': 'https://docs.askthecode.ai', 'githubUrl': 'https://github.com/askthecode/documentation', 'twitterUrl': 'https://twitter.com/askthecode_ai'}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Test the functions\n",
    "\"\"\"\n",
    "# Structure retrieval\n",
    "test_repo_structure_url = get_repo_structure(url=\"https://github.com/recommenders-team/recommenders\")\n",
    "print(\"Here is the structure of the repository:\")\n",
    "print(test_repo_structure_url)\n",
    "\n",
    "# Content retrieval\n",
    "test_repo_content_url = get_repo_content(url=\"https://github.com/recommenders-team/recommenders\", filePaths=[\".github/.codecov.yml\"])\n",
    "print(\"Here is the content of the repository:\")\n",
    "print(test_repo_content_url)\n",
    "\n",
    "# Branches retrieval\n",
    "test_repo_branches_url = get_repo_branches(url=\"https://github.com/RecandChat/CodeCompass\")\n",
    "print(\"Here are the branches of the repository:\")\n",
    "print(test_repo_branches_url)\n",
    "\n",
    "# Commit history retrieval\n",
    "test_commit_history_url = get_commit_history(url=\"https://github.com/recommenders-team/recommenders\", filePath=\".devcontainer/devcontainer.json\")\n",
    "print(\"Here is the commit history of the repository:\")\n",
    "print(test_commit_history_url)\n",
    "\n",
    "# Code search\n",
    "search_keywords = [\"Neural News Recommendation\"]\n",
    "test_search_repo_code_url = search_repo_code(url=\"https://github.com/recommenders-team/recommenders\", searchKeywords=search_keywords)\n",
    "print(\"Here is the search result:\")\n",
    "print(test_search_repo_code_url)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assistant API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_repo_structure\",\n",
    "            \"description\": \"Retrieves the Github repository file structure to analyze it and be able to query only relevant files. If the provided URL contains specific branch and directory information, prioritize using that over querying the entire repository structure.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"minLength\": 1,\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.\",\n",
    "                    },\n",
    "                    \"branch\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Repository branch. Provide only if user has explicitly specified it or the previous plugin response contains it.\",\n",
    "                        \"nullable\": True\n",
    "                    },\n",
    "                    \"relativePaths\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                        \"description\": \"Relative paths to retrieve. USE only paths you are certain that exist, NEVER invent them. If the provided URL contains a specific directory path, extract and use it. Otherwise, this should be a directory path or pattern only. Patterns accept * symbol as 'any substring'\",\n",
    "                        \"nullable\": True\n",
    "                    }\n",
    "            },\n",
    "            \"required\": [\"url\"],\n",
    "            \"additionalProperties\": False}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_repo_content\",\n",
    "        \"description\": \"Retrieves github repository file contents, possibly filtered by file names. Line numbers can be specified in URL as well. NEVER query this endpoint without previously querying get_repo_structure endpoint and when the next step is set to get_repo_structure.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"url\": {\n",
    "                    \"minLength\": 1,\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.\"\n",
    "                },\n",
    "                \"branch\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Repository branch. Provide only if user has explicitly specified it or the previous assistant response contains it. When requesting file from commit, use commit SHA.\",\n",
    "                    \"nullable\": True\n",
    "                },\n",
    "                \"relativePath\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Relative paths to the directory. Provide only if user has explicitly specified it or the previous plugin response contains it.\",\n",
    "                    \"nullable\": True\n",
    "                },\n",
    "                \"filePaths\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"description\": \"Files to query the content of. Order them by relevance descendant. This should NEVER contain the repository branch. First determine the branch if possible, and only then the file paths. Pass only if you are sure about the file path, call get_repo_structure otherwise\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"url\", \"filePaths\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "},\n",
    "{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_repo_branches\",\n",
    "        \"description\": \"Retrieves a list of branches from a Github repository given its URL.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"url\": {\n",
    "                    \"minLength\": 1,\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"url\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "},\n",
    "{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_commit_history\",\n",
    "        \"description\": \"Returns the commits history for the specific file in the repository. If the file path is not provided, the history of the entire repository will be returned. If the branch is not provided, the default branch will be used.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"url\": {\n",
    "                    \"minLength\": 1,\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.\"\n",
    "                },\n",
    "                \"branch\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Repository branch. Provide only if user has explicitly specified it or the previous assistant response contains it.\",\n",
    "                    \"nullable\": True\n",
    "                },\n",
    "                \"filePath\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Path to the file to request the commit history for. Use path relative to the root directory of the repository.\",\n",
    "                    \"nullable\": True\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"url\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "},\n",
    "{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_repo_code\",\n",
    "            \"description\": \"Search code by user specified keywords. Use when user explicitly asked to search for something. Otherwise prefer to fetch the repository structure. Invoke only with user-specified, specific keywords (e.g., file, class, method names). Avoid generic terms.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"minLength\": 1,\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.\"\n",
    "                    },\n",
    "                    \"searchKeywords\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                        \"description\": \"Search keywords. Invoke only with user-specified keywords. Never use keywords that are not part of the user prompt. When user asks to search for function definitions in a specific file (not directory) and you cannot parse them from file content, pass function keyword relevant for the file language..\"\n",
    "                    },\n",
    "                    \"branch\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Repository branch. Provide only if user has explicitly specified it or the previous plugin response contains it. When requesting file from commit, use commit SHA.\",\n",
    "                        \"nullable\": True\n",
    "                    },\n",
    "                    \"relativePath\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Relative path to the file or directory to search in. Provide only if user has explicitly specified it or the previous plugin response contains it.\",\n",
    "                        \"nullable\": True\n",
    "                    },\n",
    "                    \"searchHitLinesCount\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Number of lines to retrieve. Set only when explicitly asked to retrieve the specified amount of lines by the user.\",\n",
    "                        \"format\": \"int32\",\n",
    "                        \"nullable\": True\n",
    "                    },\n",
    "                    \"skipMatchesCount\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Number of matches to skip in the file. use only when user is searching over file and you need to search for matches that were omitted from the previous search request\",\n",
    "                        \"format\": \"int32\",\n",
    "                        \"nullable\": True\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\", \"searchKeywords\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "# Add more tools here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name = \"codecompass\",\n",
    "    instructions = \"You are a helpful assistant that analyzes code from github repositories and files when given a github url. You will answer questions about the structure of a repository, the content of a files, or any other code-related queries.\",\n",
    "    model = \"gpt-3.5-turbo-0125\",\n",
    "    tools = tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a message and run\n",
    "\n",
    "def create_message_and_run(assistant,query,thread=None):\n",
    "  if not thread:\n",
    "    thread = client.beta.threads.create()\n",
    "\n",
    "  message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=query\n",
    "  )\n",
    "  run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id\n",
    "  )\n",
    "  return run,thread\n",
    "\n",
    "# Utility function to get details of function to be called\n",
    "\n",
    "def get_function_details(run):\n",
    "\n",
    "  print(\"\\nrun.required_action\\n\",run.required_action)\n",
    "\n",
    "  function_name = run.required_action.submit_tool_outputs.tool_calls[0].function.name\n",
    "  arguments = run.required_action.submit_tool_outputs.tool_calls[0].function.arguments\n",
    "  function_id = run.required_action.submit_tool_outputs.tool_calls[0].id\n",
    "\n",
    "  print(f\"function_name: {function_name} and arguments: {arguments}\")\n",
    "\n",
    "  return function_name, arguments, function_id\n",
    "\n",
    "# Utility function to submit the function response\n",
    "\n",
    "def submit_tool_outputs(run,thread,function_id,function_response):\n",
    "    run = client.beta.threads.runs.submit_tool_outputs(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id,\n",
    "    tool_outputs=[\n",
    "      {\n",
    "        \"tool_call_id\": function_id,\n",
    "        \"output\": str(function_response),\n",
    "      }\n",
    "    ]\n",
    "    ) \n",
    "    return run\n",
    "\n",
    "available_functions = {\n",
    "    \"get_repo_structure\": get_repo_structure,\n",
    "    \"get_repo_content\": get_repo_content,\n",
    "    \"get_repo_branches\": get_repo_branches,\n",
    "    \"get_commit_history\": get_commit_history\n",
    "}\n",
    "\n",
    "'''''\n",
    "def get_gpt_response(messages):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model = \"gpt-3.5-turbo-0125\",\n",
    "        messages = messages,\n",
    "        functions = tools,\n",
    "        function_call = \"auto\"\n",
    "    )\n",
    "    return chat_completion\n",
    "'''''\n",
    "\n",
    "# execute the function\n",
    "\n",
    "def execute_function_call(function_name,arguments):\n",
    "    function = available_functions.get(function_name,None)\n",
    "    if function:\n",
    "        arguments = json.loads(arguments)\n",
    "        results = function(**arguments)\n",
    "    else:\n",
    "        results = f\"Error: function {function_name} does not exist\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "query = \"I want to know about my repository\"\n",
    "run,thread = create_message_and_run(assistant = assistant ,query = query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Run(id='run_csiOafvzGiuJcmeVaUkfTwG5', assistant_id='asst_Xwv6OQ2NEREDCHeRuxgyAeT5', cancelled_at=None, completed_at=None, created_at=1711483171, expires_at=1711483771, failed_at=None, file_ids=[], instructions='You are a helpful assistant that analyzes code from github repositories and files when given a github url. You will answer questions about the structure of a repository, the content of a files, or any other code-related queries.', last_error=None, metadata={}, model='gpt-3.5-turbo-0125', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_fXbGGI9cj8fLgObFxBdzzU01', tools=[FunctionTool(function=FunctionDefinition(name='get_repo_structure', description='Retrieves the Github repository file structure to analyze it and be able to query only relevant files. If the provided URL contains specific branch and directory information, prioritize using that over querying the entire repository structure.', parameters={'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}, 'branch': {'type': 'string', 'description': 'Repository branch. Provide only if user has explicitly specified it or the previous plugin response contains it.', 'nullable': True}, 'relativePaths': {'type': 'array', 'items': {'type': 'string'}, 'description': \"Relative paths to retrieve. USE only paths you are certain that exist, NEVER invent them. If the provided URL contains a specific directory path, extract and use it. Otherwise, this should be a directory path or pattern only. Patterns accept * symbol as 'any substring'\", 'nullable': True}}, 'required': ['url'], 'additionalProperties': False}), type='function'), FunctionTool(function=FunctionDefinition(name='get_repo_content', description='Retrieves github repository file contents, possibly filtered by file names. Line numbers can be specified in URL as well. NEVER query this endpoint without previously querying get_repo_structure endpoint and when the next step is set to get_repo_structure.', parameters={'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}, 'branch': {'type': 'string', 'description': 'Repository branch. Provide only if user has explicitly specified it or the previous assistant response contains it. When requesting file from commit, use commit SHA.', 'nullable': True}, 'relativePath': {'type': 'string', 'description': 'Relative paths to the directory. Provide only if user has explicitly specified it or the previous plugin response contains it.', 'nullable': True}, 'filePaths': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Files to query the content of. Order them by relevance descendant. This should NEVER contain the repository branch. First determine the branch if possible, and only then the file paths. Pass only if you are sure about the file path, call get_repo_structure otherwise'}}, 'required': ['url', 'filePaths'], 'additionalProperties': False}), type='function'), FunctionTool(function=FunctionDefinition(name='get_repo_branches', description='Retrieves a list of branches from a Github repository given its URL.', parameters={'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}}, 'required': ['url'], 'additionalProperties': False}), type='function'), FunctionTool(function=FunctionDefinition(name='get_commit_history', description='Returns the commits history for the specific file in the repository. If the file path is not provided, the history of the entire repository will be returned. If the branch is not provided, the default branch will be used.', parameters={'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}, 'branch': {'type': 'string', 'description': 'Repository branch. Provide only if user has explicitly specified it or the previous assistant response contains it.', 'nullable': True}, 'filePath': {'type': 'string', 'description': 'Path to the file to request the commit history for. Use path relative to the root directory of the repository.', 'nullable': True}}, 'required': ['url'], 'additionalProperties': False}), type='function'), FunctionTool(function=FunctionDefinition(name='search_repo_code', description='Search code by user specified keywords. Use when user explicitly asked to search for something. Otherwise prefer to fetch the repository structure. Invoke only with user-specified, specific keywords (e.g., file, class, method names). Avoid generic terms.', parameters={'type': 'object', 'properties': {'url': {'minLength': 1, 'type': 'string', 'description': 'Full Github repository URL provided by the user. For example: https://github.com/[owner]/[repo]/blob/[branch]/[file-path]#[additional-parameters]. The URL MUST be identical to the one, that was provided by the user, you MUST NEVER alter or truncate it. This is crucial for valid responses. You should NEVER truncate additional-parameters.'}, 'searchKeywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Search keywords. Invoke only with user-specified keywords. Never use keywords that are not part of the user prompt. When user asks to search for function definitions in a specific file (not directory) and you cannot parse them from file content, pass function keyword relevant for the file language..'}, 'branch': {'type': 'string', 'description': 'Repository branch. Provide only if user has explicitly specified it or the previous plugin response contains it. When requesting file from commit, use commit SHA.', 'nullable': True}, 'relativePath': {'type': 'string', 'description': 'Relative path to the file or directory to search in. Provide only if user has explicitly specified it or the previous plugin response contains it.', 'nullable': True}, 'searchHitLinesCount': {'type': 'integer', 'description': 'Number of lines to retrieve. Set only when explicitly asked to retrieve the specified amount of lines by the user.', 'format': 'int32', 'nullable': True}, 'skipMatchesCount': {'type': 'integer', 'description': 'Number of matches to skip in the file. use only when user is searching over file and you need to search for matches that were omitted from the previous search request', 'format': 'int32', 'nullable': True}}, 'required': ['url', 'searchKeywords'], 'additionalProperties': False}), type='function')], usage=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run status in_progress\n",
      "run status in_progress\n",
      "run status requires_action\n",
      "\n",
      "run.required_action\n",
      " RequiredAction(submit_tool_outputs=RequiredActionSubmitToolOutputs(tool_calls=[RequiredActionFunctionToolCall(id='call_f94dJjcJFjWD5BVK9lDna791', function=Function(arguments='{\"url\":\"https://github.com/eniallator/call-me-dumb\"}', name='get_repo_structure'), type='function')]), type='submit_tool_outputs')\n",
      "function_name: get_repo_structure and arguments: {\"url\":\"https://github.com/eniallator/call-me-dumb\"}\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status completed\n",
      "I encountered an error while trying to retrieve the requested repository or branch. Please verify that the provided URL is valid. If the repository is private, make sure to use GitHub authentication when using the AskTheCode plugin, and ensure that the account being used has access to the requested repository. You can find more information on our [website](https://askthecode.ai) and [documentation](https://docs.askthecode.ai). Feel free to reach out if you need further assistance!\n",
      "run status in_progress\n",
      "run status requires_action\n",
      "\n",
      "run.required_action\n",
      " RequiredAction(submit_tool_outputs=RequiredActionSubmitToolOutputs(tool_calls=[RequiredActionFunctionToolCall(id='call_4MZGlVWGEwdoPOT4c8PUOx2z', function=Function(arguments='{\"url\":\"https://github.com/recommenders-team/recommenders\"}', name='get_repo_structure'), type='function')]), type='submit_tool_outputs')\n",
      "function_name: get_repo_structure and arguments: {\"url\":\"https://github.com/recommenders-team/recommenders\"}\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status completed\n",
      "The repository \"recommenders\" by recommenders-team contains a variety of files and directories. Some of the key files and directories in the repository include:\n",
      "\n",
      "1. README.md: Contains information about the repository and how to get started.\n",
      "2. LICENSE: The license under which the repository is distributed.\n",
      "3. setup.py: Python package setup file.\n",
      "4. examples/: Directory containing example notebooks for quick start and different models.\n",
      "5. recommenders/: Directory containing code for datasets, evaluation, models, tuning, and utilities.\n",
      "6. tests/: Directory containing unit tests, functional tests, performance tests, smoke tests, and data validation tests.\n",
      "7. tools/: Directory containing tools related to the repository.\n",
      "8. docs/: Directory containing documentation files.\n",
      "\n",
      "If you would like to know more details about specific files or directories within the repository, feel free to ask!\n",
      "run status in_progress\n",
      "run status requires_action\n",
      "\n",
      "run.required_action\n",
      " RequiredAction(submit_tool_outputs=RequiredActionSubmitToolOutputs(tool_calls=[RequiredActionFunctionToolCall(id='call_Un0FUk1MUqYz0XiPG3Jk34Sn', function=Function(arguments='{\"url\":\"https://github.com/recommenders-team/recommenders\",\"searchKeywords\":[\"Neural News Recommendation\"],\"relativePath\":\"recommenders/models/newsrec\"}', name='search_repo_code'), type='function')]), type='submit_tool_outputs')\n",
      "function_name: search_repo_code and arguments: {\"url\":\"https://github.com/recommenders-team/recommenders\",\"searchKeywords\":[\"Neural News Recommendation\"],\"relativePath\":\"recommenders/models/newsrec\"}\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status requires_action\n",
      "\n",
      "run.required_action\n",
      " RequiredAction(submit_tool_outputs=RequiredActionSubmitToolOutputs(tool_calls=[RequiredActionFunctionToolCall(id='call_csMGPIiHRUCMjDH5hKhlKuX7', function=Function(arguments='{\"url\":\"https://github.com/recommenders-team/recommenders\",\"searchKeywords\":[\"Neural News Recommendation\"],\"relativePath\":\"recommenders/models/newsrec\"}', name='search_repo_code'), type='function')]), type='submit_tool_outputs')\n",
      "function_name: search_repo_code and arguments: {\"url\":\"https://github.com/recommenders-team/recommenders\",\"searchKeywords\":[\"Neural News Recommendation\"],\"relativePath\":\"recommenders/models/newsrec\"}\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status requires_action\n",
      "\n",
      "run.required_action\n",
      " RequiredAction(submit_tool_outputs=RequiredActionSubmitToolOutputs(tool_calls=[RequiredActionFunctionToolCall(id='call_yDxMpVug3noIAynWl6Ke9BZD', function=Function(arguments='{\"url\":\"https://github.com/recommenders-team/recommenders\",\"searchKeywords\":[\"Neural News Recommendation\"]}', name='search_repo_code'), type='function')]), type='submit_tool_outputs')\n",
      "function_name: search_repo_code and arguments: {\"url\":\"https://github.com/recommenders-team/recommenders\",\"searchKeywords\":[\"Neural News Recommendation\"]}\n",
      "run status in_progress\n",
      "run status completed\n",
      "I apologize for the inconvenience. It seems there is an issue with the search function at the moment. If you have any specific file names or keywords related to \"Neural News Recommendation\" that you would like me to search for manually in the repository, please provide them, and I will assist you further.\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status requires_action\n",
      "\n",
      "run.required_action\n",
      " RequiredAction(submit_tool_outputs=RequiredActionSubmitToolOutputs(tool_calls=[RequiredActionFunctionToolCall(id='call_IUTz8YGCcpLlglunJ92xEAeZ', function=Function(arguments='{\"url\":\"https://github.com/recommenders-team/recommenders\",\"filePaths\":[\"recommenders/models/newsrec\"]}', name='get_repo_content'), type='function')]), type='submit_tool_outputs')\n",
      "function_name: get_repo_content and arguments: {\"url\":\"https://github.com/recommenders-team/recommenders\",\"filePaths\":[\"recommenders/models/newsrec\"]}\n",
      "run status in_progress\n",
      "run status in_progress\n",
      "run status completed\n",
      "The file \"newsrec\" does not exist in the repository. If there are specific files related to news recommendation that you are looking for, please provide the file names or specific keywords, and I can try searching for them. Feel free to provide more details or let me know if there is anything else you would like to know about the repository.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id) # retrieve the run status from the thread we created in the previous cell\n",
    "    print(\"run status\", run.status)\n",
    "\n",
    "    if run.status==\"requires_action\":\n",
    "\n",
    "        function_name, arguments, function_id  = get_function_details(run)\n",
    "\n",
    "        function_response = execute_function_call(function_name,arguments)\n",
    "\n",
    "        run = submit_tool_outputs(run,thread,function_id,function_response)\n",
    "\n",
    "        continue\n",
    "    if run.status==\"completed\": # means gpt has an output\n",
    "\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        latest_message = messages.data[0]\n",
    "        text = latest_message.content[0].text.value\n",
    "        print(text)\n",
    "\n",
    "        user_input = input()\n",
    "        if user_input == \"STOP\":\n",
    "          break\n",
    "\n",
    "        run,thread = create_message_and_run(assistant=assistant,query=user_input,thread=thread)\n",
    "\n",
    "        continue;\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
