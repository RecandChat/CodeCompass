{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import openai\n",
    "import category_encoders as ce\n",
    "from tempfile import TemporaryDirectory\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "# go up two directories\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from codecompasslib.API.drive_operations import get_creds_drive, list_shared_drive_contents, download_csv_as_pd_dataframe, upload_df_to_drive_as_csv\n",
    "from codecompasslib.API.get_bulk_data import get_stared_repos, get_user_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files in the folder:\n",
      "df_embedded_3103.csv (1V7P-bjQCLmFg_7ffG-s-caI6Il6B7Zvp)\n",
      "test.csv (1hAP9CD6iP4FSZP4RSRm2CYUrS2KF_Lhf)\n",
      "Embedded_Shell_dataset.csv (11emrksL5Wtkxz74F4ZNG7aVGlFEO9CJ5)\n",
      "Embedded_Ruby_dataset.csv (1NqgktN6-inwI2kjhOZcqK2z8yPMQGz1I)\n",
      "Embedded_Python_dataset.csv (1eIRiKdPDhyJdWyXmo0sI-x8AfA_qEhCl)\n",
      "Embedded_PHP_dataset.csv (1DPOA1sTfewo1J9-y9ScW8_Br_2xwFBDE)\n",
      "Embedded_Jupyter Notebook_dataset.csv (1-5LumCPIn9zSQOz2B95YjxHE3CSsF6u4)\n",
      "Embedded_Java_dataset.csv (17P4T41NxcBlJ4d5ZRp-TiMHIDC9GPM4N)\n",
      "Embedded_JavaScript_dataset.csv (1rOTrBEO3jpTs8O8XHeH87N_lly8l96CV)\n",
      "Embedded_C_dataset.csv (1J4Ke7ovrVArP9gN99gRNEFwyt64drNre)\n",
      "Embedded_C++_dataset.csv (1exN3p8ElxD_rDFvKPh1Ojf2Az4zV4NF1)\n",
      "Embedded_C#_dataset.csv (1LbSVpDfCi-6f2uz0w-R05wPatNI0o1p7)\n",
      "df_with_embeddings.csv (1ob1LmG5vjvkbhhx7ZHkimXFMNXOvSpMq)\n",
      "uploaded_dataset.csv (1WSgwAhzNbSqC6e_RRBDHpgpQCnGZvVcc)\n",
      "dataset.csv (1AdJGrqauyeOzjyaT0752hTJrkQjbEvfb)\n",
      "allReposCleaned.csv (1jIYBQQJNo2s1bo3LHlYgKzUNNM0ueuhQ)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DRIVE_ID = \"0AL1DtB4TdEWdUk9PVA\"\n",
    "DATA_FOLDER = \"13JitBJQLNgMvFwx4QJcvrmDwKOYAShVx\"\n",
    "\n",
    "creds = get_creds_drive()\n",
    "list_shared_drive_contents(creds=creds, folder_id=DATA_FOLDER, drive_id=DRIVE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download 11%.\n",
      "\n",
      "Download 23%.\n",
      "\n",
      "Download 35%.\n",
      "\n",
      "Download 47%.\n",
      "\n",
      "Download 59%.\n",
      "\n",
      "Download 71%.\n",
      "\n",
      "Download 83%.\n",
      "\n",
      "Download 95%.\n",
      "\n",
      "Download 100%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ketis\\UniversityStuff\\2024\\RecAndChat\\CodeCompass\\codecompasslib\\models\\examples\\../../..\\codecompasslib\\API\\drive_operations.py:88: DtypeWarning: Columns (6,11,12,15,16,17,18,19,20,21,22,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return read_csv(fh)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download 4%.\n",
      "\n",
      "Download 8%.\n",
      "\n",
      "Download 12%.\n",
      "\n",
      "Download 16%.\n",
      "\n",
      "Download 20%.\n",
      "\n",
      "Download 24%.\n",
      "\n",
      "Download 28%.\n",
      "\n",
      "Download 32%.\n",
      "\n",
      "Download 36%.\n",
      "\n",
      "Download 40%.\n",
      "\n",
      "Download 45%.\n",
      "\n",
      "Download 49%.\n",
      "\n",
      "Download 53%.\n",
      "\n",
      "Download 57%.\n",
      "\n",
      "Download 61%.\n",
      "\n",
      "Download 65%.\n",
      "\n",
      "Download 69%.\n",
      "\n",
      "Download 73%.\n",
      "\n",
      "Download 77%.\n",
      "\n",
      "Download 81%.\n",
      "\n",
      "Download 86%.\n",
      "\n",
      "Download 90%.\n",
      "\n",
      "Download 94%.\n",
      "\n",
      "Download 98%.\n",
      "\n",
      "Download 100%.\n"
     ]
    }
   ],
   "source": [
    "# Embedded dataset is big and has slow retrieval, waiting for maud for best database options for faster retrieval\n",
    "\n",
    "# Load embedded and non-embedded dataset\n",
    "df_non_embedded = download_csv_as_pd_dataframe(creds=creds, file_id=\"1WSgwAhzNbSqC6e_RRBDHpgpQCnGZvVcc\")\n",
    "df_embedded = download_csv_as_pd_dataframe(creds=creds, file_id=\"1V7P-bjQCLmFg_7ffG-s-caI6Il6B7Zvp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep the dataframe for lightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting starred repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a target user to generate recommendations for\n",
    "target_user = 'Rameshwar0852'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "starred_by_target = get_stared_repos(target_user)\n",
    "starred_repo_ids = ids = [item['id'] for item in starred_by_target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding stars column to the embedded dataset (add any other column if you want to use it for a model)\n",
    "df_merged = pd.merge(df_embedded, df_non_embedded[['id', 'stars', 'language']], on='id', how='left')\n",
    "# turn stars column into integer column\n",
    "df_merged['stars'] = df_merged['stars'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add target column which will be 1 if the user has starred the repo and 0 otherwise\n",
    "df_merged['target'] = df_merged['id'].apply(lambda x: 1 if x in starred_repo_ids else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train lightGBM model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEAF = 64\n",
    "MIN_DATA = 20\n",
    "NUM_OF_TREES = 100\n",
    "TREE_LEARNING_RATE = 0.15\n",
    "EARLY_STOPPING_ROUNDS = 20\n",
    "METRIC = \"auc\"\n",
    "SIZE = \"sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"task\": \"train\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_class\": 1,\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": METRIC,\n",
    "    \"num_leaves\": MAX_LEAF,\n",
    "    \"min_data\": MIN_DATA,\n",
    "    \"boost_from_average\": True,\n",
    "    # set it according to your cpu cores.\n",
    "    \"num_threads\": 20,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"learning_rate\": TREE_LEARNING_RATE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_merged.drop(columns=['target', 'id', 'owner_user'])\n",
    "y = df_merged['target']\n",
    "\n",
    "X_combined, X_test, y_combined, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, y_combined, test_size=0.1, random_state=42, stratify=y_combined)\n",
    "\n",
    "# combine X_train and y_train\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "valid_data = pd.concat([X_val, y_val], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nume_cols = [\"embedding_\" + str(i) for i in range(256)] + [\"stars\"]\n",
    "cate_cols = [\"language\"]\t\n",
    "label_col = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: X: (974068, 258); Y: (974068,).\n",
      "Valid Data Shape: X: (108230, 258); Y: (108230,).\n",
      "Test Data Shape: X: (120256, 258); Y: (120256,).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_248</th>\n",
       "      <th>embedding_249</th>\n",
       "      <th>embedding_250</th>\n",
       "      <th>embedding_251</th>\n",
       "      <th>embedding_252</th>\n",
       "      <th>embedding_253</th>\n",
       "      <th>embedding_254</th>\n",
       "      <th>embedding_255</th>\n",
       "      <th>stars</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66436</th>\n",
       "      <td>-0.02661</td>\n",
       "      <td>-0.03397</td>\n",
       "      <td>-0.05840</td>\n",
       "      <td>0.00207</td>\n",
       "      <td>-0.084960</td>\n",
       "      <td>-0.07605</td>\n",
       "      <td>-0.02669</td>\n",
       "      <td>0.10345</td>\n",
       "      <td>-0.05478</td>\n",
       "      <td>0.15160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010445</td>\n",
       "      <td>-0.13370</td>\n",
       "      <td>-0.12900</td>\n",
       "      <td>0.00489</td>\n",
       "      <td>0.02968</td>\n",
       "      <td>0.06305</td>\n",
       "      <td>-0.05823</td>\n",
       "      <td>0.03165</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70728</th>\n",
       "      <td>-0.06226</td>\n",
       "      <td>-0.08770</td>\n",
       "      <td>-0.03235</td>\n",
       "      <td>0.07570</td>\n",
       "      <td>0.007385</td>\n",
       "      <td>0.10840</td>\n",
       "      <td>-0.01454</td>\n",
       "      <td>0.17330</td>\n",
       "      <td>0.01468</td>\n",
       "      <td>-0.01625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.04074</td>\n",
       "      <td>-0.07135</td>\n",
       "      <td>-0.03992</td>\n",
       "      <td>-0.03546</td>\n",
       "      <td>0.08430</td>\n",
       "      <td>-0.07690</td>\n",
       "      <td>0.01101</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275250</th>\n",
       "      <td>-0.06550</td>\n",
       "      <td>-0.09564</td>\n",
       "      <td>-0.05142</td>\n",
       "      <td>-0.04962</td>\n",
       "      <td>-0.029590</td>\n",
       "      <td>-0.06810</td>\n",
       "      <td>-0.02458</td>\n",
       "      <td>0.10300</td>\n",
       "      <td>-0.05774</td>\n",
       "      <td>0.15040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055200</td>\n",
       "      <td>-0.03290</td>\n",
       "      <td>-0.02280</td>\n",
       "      <td>0.01950</td>\n",
       "      <td>-0.04843</td>\n",
       "      <td>-0.03700</td>\n",
       "      <td>-0.04420</td>\n",
       "      <td>0.09300</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825590</th>\n",
       "      <td>-0.05090</td>\n",
       "      <td>0.03032</td>\n",
       "      <td>-0.02420</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>-0.000988</td>\n",
       "      <td>-0.05905</td>\n",
       "      <td>0.04416</td>\n",
       "      <td>0.01932</td>\n",
       "      <td>-0.00618</td>\n",
       "      <td>0.12274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040370</td>\n",
       "      <td>-0.07117</td>\n",
       "      <td>-0.02058</td>\n",
       "      <td>0.02441</td>\n",
       "      <td>-0.05270</td>\n",
       "      <td>-0.05798</td>\n",
       "      <td>0.04056</td>\n",
       "      <td>0.04953</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725083</th>\n",
       "      <td>-0.01985</td>\n",
       "      <td>-0.02382</td>\n",
       "      <td>-0.05206</td>\n",
       "      <td>0.03555</td>\n",
       "      <td>0.035580</td>\n",
       "      <td>-0.01424</td>\n",
       "      <td>-0.12490</td>\n",
       "      <td>0.13350</td>\n",
       "      <td>-0.02283</td>\n",
       "      <td>-0.03394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-0.12115</td>\n",
       "      <td>-0.04892</td>\n",
       "      <td>-0.09045</td>\n",
       "      <td>0.03032</td>\n",
       "      <td>-0.03840</td>\n",
       "      <td>-0.06580</td>\n",
       "      <td>0.06305</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "66436      -0.02661     -0.03397     -0.05840      0.00207    -0.084960   \n",
       "70728      -0.06226     -0.08770     -0.03235      0.07570     0.007385   \n",
       "275250     -0.06550     -0.09564     -0.05142     -0.04962    -0.029590   \n",
       "825590     -0.05090      0.03032     -0.02420      0.06540    -0.000988   \n",
       "725083     -0.01985     -0.02382     -0.05206      0.03555     0.035580   \n",
       "\n",
       "        embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "66436      -0.07605     -0.02669      0.10345     -0.05478      0.15160  ...   \n",
       "70728       0.10840     -0.01454      0.17330      0.01468     -0.01625  ...   \n",
       "275250     -0.06810     -0.02458      0.10300     -0.05774      0.15040  ...   \n",
       "825590     -0.05905      0.04416      0.01932     -0.00618      0.12274  ...   \n",
       "725083     -0.01424     -0.12490      0.13350     -0.02283     -0.03394  ...   \n",
       "\n",
       "        embedding_248  embedding_249  embedding_250  embedding_251  \\\n",
       "66436       -0.010445       -0.13370       -0.12900        0.00489   \n",
       "70728        0.058900        0.04074       -0.07135       -0.03992   \n",
       "275250      -0.055200       -0.03290       -0.02280        0.01950   \n",
       "825590       0.040370       -0.07117       -0.02058        0.02441   \n",
       "725083       0.080000       -0.12115       -0.04892       -0.09045   \n",
       "\n",
       "        embedding_252  embedding_253  embedding_254  embedding_255  stars  \\\n",
       "66436         0.02968        0.06305       -0.05823        0.03165      0   \n",
       "70728        -0.03546        0.08430       -0.07690        0.01101      2   \n",
       "275250       -0.04843       -0.03700       -0.04420        0.09300      1   \n",
       "825590       -0.05270       -0.05798        0.04056        0.04953      0   \n",
       "725083        0.03032       -0.03840       -0.06580        0.06305      0   \n",
       "\n",
       "        language  \n",
       "66436          1  \n",
       "70728          2  \n",
       "275250         1  \n",
       "825590         3  \n",
       "725083         1  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_encoder = ce.ordinal.OrdinalEncoder(cols=cate_cols)\n",
    "\n",
    "def encode_csv(df, encoder, label_col, typ=\"fit\"):\n",
    "    if typ == \"fit\":\n",
    "        df = encoder.fit_transform(df)\n",
    "    else:\n",
    "        df = encoder.transform(df)\n",
    "    y = df[label_col].values\n",
    "    del df[label_col]\n",
    "    return df, y\n",
    "\n",
    "train_x, train_y = encode_csv(train_data, ord_encoder, label_col)\n",
    "valid_x, valid_y = encode_csv(valid_data, ord_encoder, label_col, \"transform\")\n",
    "test_x, test_y = encode_csv(test_data, ord_encoder, label_col, \"transform\")\n",
    "\n",
    "print(\"Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n\"\n",
    "      .format(trn_x_shape=train_x.shape,\n",
    "              trn_y_shape=train_y.shape,\n",
    "              vld_x_shape=valid_x.shape,\n",
    "              vld_y_shape=valid_y.shape,\n",
    "              tst_x_shape=test_x.shape,\n",
    "              tst_y_shape=test_y.shape,))\n",
    "\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6, number of negative: 974062\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.438007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65775\n",
      "[LightGBM] [Info] Number of data points in the train set: 974068, number of used features: 258\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000006 -> initscore=-11.997471\n",
      "[LightGBM] [Info] Start training from score -11.997471\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's auc: 0.932366\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params, categorical_feature=cate_cols)\n",
    "lgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "lgb_test = lgb.Dataset(test_x, test_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "lgb_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=NUM_OF_TREES,\n",
    "                      valid_sets=lgb_valid,\n",
    "                      categorical_feature=cate_cols,\n",
    "                      callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.4993721674774438, 'logloss': 0.000461882507551249}\n"
     ]
    }
   ],
   "source": [
    "test_preds = lgb_model.predict(test_x)\n",
    "auc = roc_auc_score(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "logloss = log_loss(np.asarray(test_y.reshape(-1)), np.asarray(test_preds), eps=1e-12)\n",
    "res_basic = {\"auc\": auc, \"logloss\": logloss}\n",
    "print(res_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "with TemporaryDirectory() as tmp:\n",
    "    save_file = os.path.join(tmp, \"finished_LGBM.model\")\n",
    "    lgb_model.save_model(save_file)\n",
    "    loaded_model = lgb.Booster(model_file=save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for all the repos\n",
    "df_test = df_merged.drop(columns=['id', 'owner_user'])\n",
    "full_dataset_x, full_dataset_y = encode_csv(df_test, ord_encoder, label_col, \"transform\")\n",
    "all_preds = lgb_model.predict(full_dataset_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository ID: 40250091.0  | Owner: danishkhan  | Prediction: 1.0\n",
      "Repository ID: 21378463.0  | Owner: bblzjp  | Prediction: 1.0\n",
      "Repository ID: 559467658.0  | Owner: lujiacn  | Prediction: 1.0\n",
      "Repository ID: 134444559.0  | Owner: nkprince007  | Prediction: 1.0\n",
      "Repository ID: 549422245.0  | Owner: mozmut  | Prediction: 1.0\n",
      "Repository ID: 140065411.0  | Owner: soon14  | Prediction: 0.9999999999999774\n",
      "Repository ID: 574946168.0  | Owner: chiyutianyi  | Prediction: 0.9999999999766165\n",
      "Repository ID: 702245469.0  | Owner: nuzulfikrie  | Prediction: 0.9955527587034938\n",
      "Repository ID: 387313950.0  | Owner: jhon-jader  | Prediction: 0.40993307170474874\n",
      "Repository ID: 548651131.0  | Owner: mrgius3ppe  | Prediction: 0.3780267309085086\n"
     ]
    }
   ],
   "source": [
    "# get sorted predictions with highest first\n",
    "top_indices = np.argsort(all_preds)[::-1]\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "counter = 0\n",
    "for index in top_indices:\n",
    "    if counter == 10:\n",
    "        break\n",
    "    # disregard if the repo is already starred by the user\n",
    "    if df_merged.iloc[index]['id'] in starred_repo_ids:\n",
    "        continue\n",
    "    else:\n",
    "        counter += 1\n",
    "        recommendations.append((df_merged.iloc[index]['id'], df_merged.iloc[index]['owner_user'], all_preds[index]))\n",
    "        print(\"Repository ID:\", df_merged.iloc[index]['id'], \" | Owner:\", df_merged.iloc[index]['owner_user'], \" | Prediction:\", all_preds[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
