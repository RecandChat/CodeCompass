{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'codecompasslib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearestNeighbors\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecompasslib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAPI\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrive_operations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_csv_as_pd_dataframe, get_creds_drive\n\u001b[1;32m     13\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecompasslib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_word2vec_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'codecompasslib'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from codecompasslib.API.drive_operations import download_csv_as_pd_dataframe, get_creds_drive\n",
    "sys.path.append('../../')\n",
    "from codecompasslib.models.embeddings import load_word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(full_data_folder_id: str, full_data_embedded_folder_id: str) -> Tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"\n",
    "    Load the data from the Google Drive\n",
    "    :return: The non-embedded and embedded datasets\n",
    "    \"\"\"\n",
    "    DRIVE_ID = \"0AL1DtB4TdEWdUk9PVA\"\n",
    "    DATA_FOLDER = \"13JitBJQLNgMvFwx4QJcvrmDwKOYAShVx\"\n",
    "\n",
    "    creds = get_creds_drive()\n",
    "    df_non_embedded: DataFrame = download_csv_as_pd_dataframe(creds=creds, file_id=full_data_folder_id)\n",
    "    df_embedded: DataFrame = download_csv_as_pd_dataframe(creds=creds, file_id=full_data_embedded_folder_id)\n",
    "\n",
    "    # Having data locally works much faster than retrieving from drive. Uncomment the following lines to use local data\n",
    "    # df_non_embedded = pd.read_csv('codecompasslib/models/data_full.csv')\n",
    "    # df_embedded = pd.read_csv('codecompasslib/models/df_embedded_combined.csv')\n",
    "\n",
    "    print(\"Data loaded\")\n",
    "    return df_non_embedded, df_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_folder_id = '1Qiy9u03hUthqaoBDr4VQqhKwtLJ2O3Yd'\n",
    "full_data_embedded_folder_id = '139wi78iRzhwGZwxmI5WALoYocR-Rk9By'\n",
    "\n",
    "df_non_embedded, df_embedded = load_data(full_data_folder_id, full_data_embedded_folder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(df_non_embedded):\n",
    "    \"\"\"\n",
    "    Load and clean the dataset from a specified filepath.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): The file path to the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = df_non_embedded\n",
    "\n",
    "    # Delete missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Delete columns that are not needed\n",
    "    columns_to_drop = [\n",
    "        'is_archived', 'is_disabled', 'is_template', 'has_projects',  \n",
    "        'owner_type', 'has_pages', 'has_wiki', \n",
    "        'has_issues', 'has_downloads', 'is_fork'\n",
    "    ]\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    # Handling missing values in text columns\n",
    "    df['description'].fillna('', inplace=True)\n",
    "    df['name'].fillna('', inplace=True)\n",
    "    df['language'].fillna('', inplace=True)\n",
    "\n",
    "    # Drop duplicates with name\n",
    "    df.drop_duplicates(subset='name', keep='first', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_clean_data(df_non_embedded)\n",
    "\n",
    "# count unique languges\n",
    "df['language'].nunique()\n",
    "\n",
    "# Create list of unique languages with _ prefix\n",
    "languages = ['_' + language for language in df['language'].unique()]\n",
    "\n",
    "# one hot encode the languages and don't include the language prefix\n",
    "df = pd.get_dummies(df, columns=['language'], prefix='')\n",
    "\n",
    "# Turn df into a repo specific df with owner_user as a unique identifier, appending description and keeping 1 if any of the languages are present in at least one repo\n",
    "\n",
    "# Create a dictionary for aggregation\n",
    "aggregation_dict = {\n",
    "    'name': lambda x: list(x),\n",
    "    'description': lambda x: list(x)\n",
    "}\n",
    "\n",
    "# Add columns for languages\n",
    "for lang in languages:\n",
    "    aggregation_dict[lang] = 'max'\n",
    "\n",
    "# Group by 'owner_user' and aggregate\n",
    "user_df = df.groupby('owner_user').agg(aggregation_dict).reset_index()\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "user_df.head()\n",
    "\n",
    "# first we turn list of names and descriptions into a single string\n",
    "user_df['name'] = user_df['name'].apply(lambda x: ' '.join(str(i) for i in x) if isinstance(x, list) else '')\n",
    "user_df['description'] = user_df['description'].apply(lambda x: ' '.join(str(i) for i in x) if isinstance(x, list) else '')\n",
    "user_df.head()\n",
    "word_vect = load_word2vec_model\n",
    "\n",
    "# Text preprocessing\n",
    "embedded_user_df = user_df.copy()\n",
    "embedded_user_df['name'] = user_df['name'].fillna('')  \n",
    "embedded_user_df['description'] = user_df['description'].fillna('')\n",
    "\n",
    "embedded_user_df['name_vector'] = embedded_user_df['name'].apply(vectorize_text)\n",
    "embedded_user_df['description_vector'] = embedded_user_df['description'].apply(vectorize_text)\n",
    "embedded_user_df\n",
    "# embedded_user_df.drop(['name', 'description', 'owner_user'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform df into something that KNN can use. To be more specific, into a feature matrix\n",
    "# Create a list of all the vectors\n",
    "vectors = []\n",
    "repo_df = embedded_user_df * 1 # convert all boolean values in repo_df to 0 or 1\n",
    "\n",
    "for row in repo_df.index: \n",
    "    vector = []\n",
    "    for columns in ['name_vector', 'description_vector']:\n",
    "        if type(repo_df.at[row, columns]) == np.ndarray:\n",
    "            for element in repo_df.at[row, columns]:\n",
    "                vector.append(element)\n",
    "        else: vector.append(repo_df.at[row, columns])\n",
    "    vectors.append(vector)\n",
    "\n",
    "    # Train Nearest Neighbors Model\n",
    "k = 5  # Number of neighbors to find\n",
    "nn_model = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "nn_model.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "\n",
    "target_user = 21\n",
    "# neighbors excluding the target user\n",
    "neighbors = nn_model.kneighbors([vectors[target_user]], return_distance=False)[0][1:]\n",
    "neighbors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
